id,date,update,title,summary,pdf
R48319,2024-12-30T05:00:00Z,2025-06-21T04:54:22Z,Artificial Intelligence (AI) in Health Care,"The use of artificial intelligence (AI) in health care settings has grown over time in part due to recent increases in health care data availability and innovations in big data analytical methods. AI encompasses multiple technologies and is variably defined, with some disagreement at times among stakeholders about which technologies even qualify as AI. Machine learning (ML) techniques, including deep learning and neural networks, underpin many AI tools used in health care. Some of the more widely used AI techniques and applications in health care include natural language processing, which refers to the use of rule-based or ML approaches to understanding the structure and meaning of human language; rule-based expert systems; physical robots; and robotic process automation. These technologies may affect a wide range of health care stakeholders, including AI developers, researchers, health systems, payers, patients, and federal agencies.AI technologies have generally become more complex over time, progressing from rule-based expert systems to generative AI models. Within health care, the use of AI broadly falls within three categories: diagnosis and treatment, patient engagement and adherence with treatment plans, and administrative applications. While AI technologies have the potential to improve health care, they may also introduce novel challenges and exacerbate existing ones if not properly overseen. There have been a variety of federal efforts undertaken to address the use of AI in health care, including by executive orders and agency actions. Agencies within the U.S. Department of Health and Human Services (HHS) that have been active in developing relevant regulations include the Assistant Secretary for Technology Policy/Office of the National Coordinator for Health Information Technology (ASTP/ONC), the U.S. Food and Drug Administration (FDA), the Office for Civil Rights (OCR), and the Centers for Medicare & Medicaid Services (CMS). These regulatory efforts in part attempt to address challenges that may arise when AI is used in health care settings, including issues related to trust in AI technology, data access, bias, lack of transparency, privacy, scaling and integration, liability, regulatory harmonization, and environmental impact. AI is a rapidly developing and changing technology; if AI is to be used safely and effectively in health care settings, ongoing oversight and regulatory efforts may need to be addressed by Congress and relevant federal regulatory bodies. This report highlights the challenges and questions that Congress may wish to consider.",https://www.congress.gov/crs_external_products/R/PDF/R48319/R48319.3.pdf
R47997,2024-04-03T04:00:00Z,2025-06-21T03:26:11Z,Artificial Intelligence and Machine Learning in Financial Services,"The financial industry’s adoption of artificial intelligence (AI) and machine learning (ML) is evolving as financial firms employ ever greater levels of technology and automation to deliver services. Expanding on earlier models of quantitative analysis, AI/ML has often been adopted in finance to solve discrete challenges, such as maximizing profit and minimizing risk. Yet the industry’s adoption of the newer technology also occurs against perceptions that are steeped in tradition and historical financial regulation, and regulators want to ensure that the technology does not sidestep regulations frequently described as technology neutral.Technological advances in computer hardware, capacity, and data storage—which permit the collection and analysis of data—helped fuel the development and use of AI/ML technologies in finance. Unlike older algorithms that automated human-coded rules, new AI models can “learn” by themselves and make inferences and recommendations not identified by modelers in advance. This shift in technology has also enabled the use of new types of data including alternative data (i.e., data that the consumer credit bureaus do not traditionally use), unstructured data (images or social media posts, etc.), and unlabeled information data—which, when combined, extend the technologies’ uses to new financial services or products. Different parts of the financial services industry have adopted AI/ML technology to varying degrees and for various purposes. Some uses of AI/ML include powering chatbots in customer service functions, identifying investment opportunities and/or executing trades, augmenting lending models or (more sparingly) making lending decisions, and identifying and preventing fraud. The extent to which a sector or firm adopts various technologies reflects a variety of factors, including a firm’s ability to fund internal development and regulatory requirements.The increased use of AI/ML to deliver financial services has attracted attention and led to numerous policy issues and subsequent policy actions. Such policy actions culminated in (1) the establishment of a task force on AI in the 116th Congress and the more recent working group in the House Committee on Financial Services in the 118th and (2) 2019 and 2023 executive orders. The evolving legislative and regulatory framework regarding AI/ML use in finance is likely, at least in part, to influence the development of AI/ML financial services applications. Various financial regulators have indicated that regulated entities are subject to the full range of laws and regulations regardless of the technology used. Additionally, some regulators have identified regulations and issued guidance of particular relevance to financial firms employing AI/ML technologies.Financial industry policymakers face competing pressures. Financial service providers and technology companies are likely to continue adopting and promoting AI/ML to save time and money and promote accessibility, accuracy, and regulatory compliance. However, challenges and risks in the form of bias, potential for systemic risk and manipulation, affordability, and consequences for employment remain. Determining whether the existing regulatory structure is sufficient—or whether one that is more closely tailored to the technological capacities of the evolving technology is necessary—has emerged as a key consideration. Should Congress consider the legislative framework governing AI/ML in finance, industry and consumers alike will expect that it weighs the benefits of innovation with existing and potential future challenges and risks.",https://www.congress.gov/crs_external_products/R/PDF/R47997/R47997.1.pdf
LSB11251,2024-12-12T05:00:00Z,2025-06-21T04:48:11Z,Artificial Intelligence and Patent Law,"Advances in artificial intelligence (AI) have raised novel questions for U.S. patent law, just as AI has raised concerns in other branches of intellectual property law, including copyrights and the right of publicity. Committees of both the House and Senate have expressed interest in these issues.In 2024, the U.S. Patent and Trademark Office (USPTO) issued separate guidance documents addressing (1) whether inventions made using AI are patentable and (2) whether inventions about AI (i.e., new AI technologies) are patentable. This Legal Sidebar provides an overview of how USPTO guidance and case law have addressed both of these issues, differing stakeholder perspectives, and potential options for Congress. Can Inventions Made Using AI Be Patented?One important issue for patent law is whether inventions made using AI can be patented. Although U.S. patent law currently requires a human inventor and does not allow patenting of inventions made solely by AI, patents can be granted on some inventions that human inventors make with AI assistance.Current Law Governing Inventorship and Joint InventorshipThe concept of inventorship in U.S. patent law traces some of its roots to the U.S. Constitution. Congress’s authority to establish patent protections—as well as copyrights—is founded on the Constitution’s Intellectual Property (IP) Clause, which gives Congress the power “[t]o promote the Progress of Science and useful Arts, by securing for limited Times to Authors and Inventors the exclusive Right to their respective Writings and Discoveries.” Thus, the IP Clause allows Congress to give copyright and patent protections to authors and inventors, respectively. Related to this shared constitutional foundation, the question of whether inventions made using AI may be patented is analogous to the question of whether creative works made using AI may be copyrighted—a question explored in a separate Legal Sidebar.The Patent Act requires that each application for a patent be made or authorized by the inventor(s), who must make an oath or declaration that they believe they are the inventor(s). The U.S. Court of Appeals for the Federal Circuit has held that inventorship requires conception, or mentally forming a complete idea for a particular new invention that skilled persons in the relevant field would be able to put into practice without excessive experimentation. As the U.S. Supreme Court has explained, a person need not successfully reduce to practice (make or perform) an invention to be an inventor—although courts recognize that, in some situations, conception and reduction to practice may tend to occur simultaneously.If an invention has multiple (joint) inventors, all of them generally must apply jointly for the patent and make the required oath. As the Federal Circuit summarized in Pannu v. Iolab Corp., to qualify as a joint inventor, a person must (1) contribute in a “significant manner” to the invention, (2) contribute in a way that is not “insignificant in quality” compared with the whole invention, and (3) contribute more than an explanation of “well-known concepts and/or the current state of the art.” The Patent Act provides that joint inventors need not “work together or at the same time,” make the “same type or amount of contribution,” or contribute to every claim of the patent. Thaler v. Vidal: Only Human Beings Can Be InventorsIn 2022, the Federal Circuit held in Thaler v. Vidal that the Patent Act requires an inventor to be a natural person (i.e., a human being), and therefore an AI system may not be listed as the inventor on a patent. The plaintiff, Stephen Thaler, claimed that he developed an AI system that autonomously conceived of two inventions without human assistance. Thaler applied for patents on both inventions, listing the AI system (and not himself) as the inventor. USPTO determined the applications were incomplete because they failed to list a human inventor. Thaler sued for review of USPTO’s decision in the U.S. District Court for the Eastern District of Virginia, which agreed with USPTO’s conclusion. On appeal, the Federal Circuit held that an inventor must be a natural person because the Patent Act defines an inventor as an “individual,” a term that the U.S. Supreme Court has explained normally refers to a human being. Thaler argued that the term “inventor” should encompass AI systems in order to serve constitutional and statutory goals of U.S. patent law: to incentivize innovation and public disclosure of new inventions. The court, however, held that these policy concerns could not override the clear meaning of the statutory text. The holding of Thaler is currently settled law throughout the United States, as the Federal Circuit has exclusive jurisdiction over intermediate appeals of patent suits and the Supreme Court declined Thaler’s petition to hear the case. (In an analogous lawsuit also brought by Thaler, a U.S. district court held in 2023 that creative works must have a human author to be copyrighted, a decision currently on appeal to the U.S. Court of Appeals for the District of Columbia.)Thaler has also listed his AI system as the inventor on patent applications in other countries. The Board of Appeal of the European Patent Office and appellate courts in the United Kingdom, Australia, and New Zealand ruled that the inventions could not be patented in those jurisdictions since only humans may be inventors. The South Africa patent office granted Thaler’s application, but commentators note that South Africa’s laws do not define “inventor” or require substantive examination of patent applications. USPTO Guidance for AI-Assisted Inventions: Human Beings Must Make “Significant Contribution” to Obtain PatentsThe Thaler court acknowledged it did not address “whether inventions made by human beings with the assistance of AI are eligible for patent protection.” In February 2024, USPTO addressed this question in its Inventorship Guidance for AI-Assisted Inventions (the AI Inventorship Guidance). The AI Inventorship Guidance states that AI-assisted inventions may be patented so long as at least one natural person “significantly contributed” to the claimed invention. It explains that this standard is satisfied when at least one human being meets all three elements of joint inventorship under Pannu, i.e., (1) making a significant contribution to conception (2) that is not insignificant in comparison with the full invention and (3) that does not merely involve explaining well-known concepts or current knowledge. The guidance states that human contributions pertaining only to reduction to practice are not sufficient for inventorship, regardless of whether AI is involved. For patents with multiple claims, the guidance requires a human being to make a significant contribution to each claim.USPTO Director Kathi Vidal characterizes the AI Inventorship Guidance as “embracing the use of AI in innovation and focusing on the human contribution.” So long at least one human being meets the significant-contribution requirement described above, the guidance allows a patent to issue even if an AI system also makes a significant contribution—one that would make the AI system a co-inventor if it were a human being. Accordingly, the guidance acknowledges that tools, including AI systems, “may perform acts that, if performed by a human, could constitute inventorship under our laws.” The AI Inventorship Guidance provides illustrations of how the “significant contribution” test would apply to AI-assisted inventions. It states that prompting, designing, building, or training an AI system “in view of a specific problem to elicit a particular solution” might constitute a significant contribution in some cases. On the other hand, the guidance asserts that presenting an AI system with only a “general goal or research plan” would not constitute inventorship. Likewise, the guidance would not consider a person who simply owns or oversees an AI system to be the inventor of inventions created using that system. USPTO elaborated further on the guidance with a separate set of examples, one for a Transaxle for Remote Control Car and the other for Developing a Therapeutic Compound for Treating Cancer. As a practical consideration for patent applicants, the guidance notes that, as part of their existing duty to disclose information that raises a “prima facie case of unpatentability,” applicants must inform USPTO of any evidence that the named inventor’s “purported contribution(s) was made by an AI system.” USPTO states that it does not “believe” this guidance will have a “major impact” on disclosure requirements.Stakeholder Views on USPTO Guidance for AI-Assisted InventionsDozens of stakeholders filed public comments expressing a mix of support and criticism for the AI Inventorship Guidance. Some, including the American Bar Association Intellectual Property Section (ABA IP Section), argued that only humans, not AI, are capable of the mental process of conception; that AI systems are simply tools used by human inventors; and that the guidance takes an overly restrictive view of what may constitute human inventorship in the context of inventions assisted by AI compared with other tools. The ABA IP Section argues that the Pannu “significant contribution” test is inappropriate for determining whether an invention has a human inventor. By contrast, the Public Interest Patent Law Institute argues that the guidance may be too lenient toward patenting AI-assisted inventions, including where the human contribution consists solely of developing or giving prompts to AI systems. Some comments offered differing views on the new disclosure requirements, in particular. The ABA IP Section, for example, opposes the guidance to the extent it requires patent applicants to disclose their use of AI tools. The Alliance for Automotive Innovation, on the other hand, argued that USPTO should modify the guidance to include an express requirement that patent applicants explain how, if at all, AI assisted the invention process, to give USPTO a fuller record to inform fact-specific decisions.When Can Inventions in AI Technology Be Patented?Apart from questions about AI and inventorship (which may implicate a variety of technological fields), the patentability of new AI technologies themselves is another significant legal issue. Although the Patent Act broadly defines types of inventions that that may be patented, the Supreme Court has long held that “abstract ideas”—such as mathematical formulas or mental processes—may not be patented, even if they meet all the other patentability requirements (e.g., they are newly discovered, nonobvious, and adequately described in the patent application). Because some AI innovations might be characterized as nonpatentable abstract ideas, stakeholders have expressed concern that current limitations on patent eligibility could harm AI innovation. This section reviews the current law of patent-eligible subject matter and explains how these concepts may apply to AI-related inventions.Current Law Governing Patent Eligible Subject MatterPatent-eligible subject matter refers to the types of inventions that may be patented. Section 101 of the Patent Act allows patents on “any new and useful process, machine, manufacture, or composition of matter.” Although this language has a wide scope, the Supreme Court has held that it has three implicit categorical exceptions: “laws of nature, natural phenomena, and abstract ideas” are not patent-eligible. For example, Albert Einstein could not have patented his formula E = mc2 (a law of nature), nor could anyone patent a newly discovered wild plant (a natural phenomenon). These three types of nonpatentable discoveries are sometimes called the judicially developed exceptions to patent-eligible subject matter.While these exceptions to patent-eligible subject matter are long-standing, their effective scope has waxed and waned over time, depending on the trends in court decisions. Federal Circuit decisions in the 1990s construed the exceptions narrowly. Then, a series of Supreme Court decisions in the 2010s broadened the judicially developed exceptions to patent-eligible subject matter, effectively narrowing the scope of inventions eligible to be patented. Specifically, these Supreme Court cases rejected, as ineligible, patents on (1) a business method for hedging price-fluctuation risk; (2) a method for calibrating the dosage of a drug to treat autoimmune diseases; (3) isolated human DNA segments; and (4) a method for mitigating settlement risk in financial transactions using a computer. The Court’s decisions established a two-step process for determining whether a patent claims ineligible subject matter, sometimes called the Alice/Mayo test or Alice/Mayo framework. The first step of the test addresses whether the patent claims are “directed to” an ineligible concept (i.e., a law of nature, a natural phenomenon, or an abstract idea). To be directed to an ineligible concept, the focus of the claims must be a patent-ineligible concept, as opposed to a technological process. If the patent claims are not directed to an ineligible concept, then the claims are patent-eligible. If the claims are directed to an ineligible concept, then the invention is not patentable unless the patent claims have an “inventive concept” under the second step of the Alice/Mayo test. Step two considers whether the elements of each patent claim contain additional aspects that “transform” it into a patent-eligible application of an ineligible concept. Claim limitations that are conventional, routine, and well understood, such as implementing an abstract idea on a generic computer, cannot supply the inventive concept.As a result of the Alice/Mayo test, fewer inventions are patentable, particularly in areas such as computer software, business methods, medical diagnostics, and biotechnology. For example, the Federal Circuit has applied Alice/Mayo to invalidate patents on a method to diagnose fetal abnormalities by detecting fetal DNA in maternal blood; a test to diagnose a neurological disorder by detecting a particular protein in bodily fluids; and a method for manufacturing driveline shafts in automotive vehicles. Particularly relevant to inventions in AI is the Court’s 2014 decision in Alice Corp. v. CLS Bank. The patents at issue in Alice concerned methods and systems for mitigating “settlement risk” (i.e., the risk that only one party in a financial transaction will pay) using a computer as a third-party intermediary. In Alice, the Court held that these inventions could not be patented because they were directed to “the abstract idea of intermediated settlement, i.e., the use of a third party to mitigate settlement risk.” Critically, the Court held that, although the method involved a computer (a “machine” under Section 101), this did not save its patent eligibility because “generic computer implementation“ was insufficient to transform an abstract idea into a patentable invention. The claims on a computer system programmed to carry out the methods failed for the same reason, as they described only “generic computer components configured to implement” the abstract idea.USPTO’s 2024 Guidance on AI and Patent EligibilityIn a 2023 executive order, President Biden directed USPTO to “clarify” issues relating to AI and patents by issuing “additional guidance to USPTO patent examiners . . . which could include, as the USPTO Director deems necessary, updated guidance on patent eligibility to address innovation in AI and critical and emerging technologies.” This order came amidst ongoing efforts by USPTO to solicit stakeholder input and study patent issues relating to AI, including a 2019 request for comments, two reports issued in 2020, and public meetings on AI issues held in 2022–2024. Among other things, USPTO’s work found that patent applications claiming AI technologies had doubled between 2002 and 2018, and that some stakeholders worried that “many AI inventions are at risk under the subject matter eligibility analysis because they can be characterized as” abstract ideas such as “certain methods of organizing human activity, mental processes, or mathematical concepts.”In 2024, USPTO issued updated guidance on patent eligibility and AI inventions (the AI Eligibility Guidance), which—combined with existing guidance—governs how patent examiners assess the eligibility of AI inventions. This guidance supplements the existing guidance that USPTO issued in 2019 to respond to the Alice/Mayo series of decisions and subsequent Federal Circuit cases. Under the 2019 guidance, the USPTO divides the first step of the Alice/Mayo test (whether the claim is “directed to” a judicial exception) into two “prongs”: (1) whether the patent claim “recites” an abstract idea, law of nature, or natural phenomena; and (2) if so, whether the claim has additional elements that “integrate” the ineligible concept into a practical application of the judicial exception. The claim is patent-eligible if it either does not recite an ineligible concept or integrates it into a practical application.The 2024 AI Eligibility Guidance focuses on these two aspects of the analysis. As to when a patent claim on an AI invention “recites” an abstract idea, the guidance gives several examples of AI inventions that are eligible because they “merely involve” abstract ideas, such as a claim on an application-specific integrated circuit (ASIC) for an artificial neural network comprising synaptic circuits, a microprocessor, and an array of “neurons” organized in a particular way. Under the guidance, this claim is patent-eligible because it is directed to specific “hardware components” and does not recite an abstract idea. In contrast, a claim on a general method of using a trained artificial neural network to detect anomalies in a data set is ineligible. The AI Eligibility Guidance explains that this claim is directed to abstract ideas (the mental process of observation and anomaly detection) implemented on generic computer components and outside any particular technological context. Other aspects of the AI Eligibility Guidance speak to when AI inventions may be eligible because they integrate an abstract idea into a practical application by “improv[ing] the functioning of a computer or improv[ing] another technology or technical field.” The guidance states that “many” AI inventions may be eligible for this reason, as when they claim “a specific application of AI to a particular technological field.” For example, the guidance explains that a general method of using a deep neural network to analyze a speech sample with multiple sources is ineligible because it claims a mathematical process. In contrast, a particular method of using a deep neural network to separate a mixed speech sample, generate separate waveforms for each speech source, and recombine them into a new mixed sample without unwanted sources is an eligible practical application of the abstract idea.Stakeholder Views on USPTO’s AI Eligibility GuidanceWhile some stakeholders appreciated USPTO’s efforts to provide more clarity on how it will approach AI patent eligibility issues, others asserted that the 2024 AI Eligibility Guidance could have done more to clarify the application of the Alice/Mayo framework to AI inventions. Several stakeholders noted that the examples in the guidance were not as helpful as they could have been, either because they involved unrealistic fact patterns or because the hypothetical claims were all either clearly eligible or clearly ineligible. That said, many commentators observed that the USPTO’s task in providing guidance in this area is “difficult” given the dearth of AI-specific case law and the ambiguities of the Alice/Mayo test itself. Some groups (such as the Council for Innovation Promotion) expressed that the guidance was “problematic” in treating trained AI as a “generic computer” and discounting AI claim limitations, and so “may serve to make obtaining patents on AI inventions unnecessarily difficult.” Other groups (such as the High Tech Inventors Alliance) had “serious concerns” with the guidance from the other direction and urged USPTO to withdraw it, arguing that the guidance’s approach to the “practical application” prong conflicts with the reasoning of the Alice decision.Considerations for CongressCongress could amend the Patent Act if it wishes to change or clarify the law of patentability for inventions made using AI, or the eligibility of inventions in the field of AI. For example, some stakeholders have expressed support for legislation expanding the scope of patent eligibility for AI technologies and other fields affected by the Alice/Mayo decisions. Introduced bills on this issue in the 118th Congress include S. 2140 and Section 7 of H.R. 8134. Congress could consider analogous legislation on inventorship, should it conclude that the AI Inventorship Guidance or subsequent case law is too lenient or too strict in allowing patents on AI-assisted inventions.Alternatively, Congress could continue to monitor how USPTO’s guidance fares in practice to decide whether new legislation is needed. USPTO acknowledges that neither its AI Inventorship Guidance nor its AI Eligibility Guidance has the force of law. Rather, these documents seek to apply the existing legal requirements that Congress has set forth in the Patent Act, as interpreted by federal courts, to guide USPTO patent examiners in evaluating patent applications. Parties may challenge USPTO’s decisions to grant or deny patent applications in federal court, and USPTO’s guidance is not legally binding on courts hearing such challenges. Ultimately, USPTO’s guidance and federal court decisions on patentability both turn principally on the interpretation of the Patent Act, which Congress may amend should it disagree with conclusions reached by USPTO or the courts concerning AI inventions.",none found
LSB11052,2024-01-29T05:00:00Z,2025-06-21T03:03:45Z,Artificial Intelligence Prompts Renewed Consideration of a Federal Right of Publicity,"Recent uses of artificial intelligence (AI) to create realistic images, videos, replicas, or voice simulations of real people have prompted some Members of Congress to call for federal legislation to protect the “right of publicity” (or ROP, for short). The ROP is often defined as the right to prevent unauthorized commercial uses of one’s name, image, or likeness (NIL) or other aspects of one’s identity (such as one’s voice). The ROP is not comprehensively protected by current federal laws. This Legal Sidebar surveys existing state-level legal protections for the ROP, explains how they intersect with federal laws regarding intellectual property (IP), describes potential ROP concerns raised by AI, and presents constitutional and other legal considerations for Congress. Another Legal Sidebar discusses questions AI raises for copyright law, while a separate Legal Sidebar and a CRS report discuss the ability of college student-athletes to receive compensation for uses of their NIL. State Right of Publicity LawsThe ROP is protected in some form by the laws of most U.S. states, and the number of states that recognize this right has expanded over the past several decades. One study found that 35 states recognized the ROP as of 2020. ROP laws generally create a private right of action for the unauthorized commercial use of another person’s NIL. For example, if a manufacturer uses a famous athlete’s name or face in a TV commercial without her permission, the athlete could sue the manufacturer for violating her ROP, and a court could order the manufacturer to pay damages and stop showing the commercial. What constitutes an unauthorized commercial use of NIL can vary from state to state. Some states’ ROP laws may apply only to advertising, while others more broadly apply to any use that commercially benefits the user, such as video game or comic book characters based on real people. Other notable differences between the ROP laws of various states concern questions such as:Is the ROP protected by statute, common law, or both? Twenty-five states have enacted statutes protecting the ROP. In some of these states, including California, the ROP is protected by both statutes and common law (law derived from court opinions). In other states, including Delaware, the ROP is protected by common law only, sometimes as an application of a common law “right of privacy.”What parts of a person’s identity does the ROP protect? The ROP often only protects a person’s NIL, voice, and signature. In some states, the ROP also includes someone’s “distinctive appearance, gestures, or mannerisms.” Courts have construed certain states’ ROP laws to protect more abstract aspects of personal identity, finding defendants liable for using an image of a famous driver’s race car, a blonde robot performing Vanna White’s role on a game show, or the catch phrase “here’s Johnny.” Do all individuals enjoy the ROP? State laws vary as to whether all persons or only those with “commercially valuable” NIL (such as celebrities) may assert ROP claims. Does the ROP survive a person’s death? In some states, the ROP is descendible, meaning it can be asserted by one’s heirs after a person dies. States that recognize a descendible (or postmortem) ROP differ as to their duration, with postmortem rights lasting 20 years in Virginia, 70 years in California, and 100 years in Oklahoma, for instance. Under Tennessee law, postmortem ROP may last indefinitely. In other states, the ROP is not descendible, or else courts have not resolved the issue.Often, the law of the state in which a person is domiciled—or, where they were domiciled when they died—governs their ROP. Indiana’s ROP statute, however, allows suit “regardless of a personality’s domicile” for infringing materials “disseminated within Indiana.” This law may allow non-Indiana plaintiffs to sue for infringing materials that are made available in Indiana via television or the internet.Intersection of Right of Publicity and Federal IP LawsAlthough the ROP is distinct from the forms of IP already protected by federal law, it is related in some ways to trademarks and copyrights. If Congress chooses to regulate the ROP via federal law, it may consider how best to harmonize the ROP with existing trademark and copyright laws.TrademarksWhile the ROP generally protects commercial uses of a person’s identity, trademarks protect commercial uses of words, names, and other symbols that distinguish one person’s goods from others. The ROP may overlap with trademarks in cases where aspects of a person’s NIL can be trademarked. A person’s name, for instance, may be trademarked if it acquires a distinctive meaning and is used commercially to identify goods or services (e.g., McDonald’s). The ROP may be seen as serving a similar function to trademarks, although some scholars have criticized the theoretical foundations and expansion of the ROP.Trademark infringement occurs when someone without authorization uses a trademark in a way that creates a likelihood of confusion for consumers. The Lanham Act—the federal trademark law—also establishes a cause of action for “false endorsement,” which provides additional protection that overlaps with the ROP. False endorsement occurs when a person’s identity is used in a way that is likely to confuse consumers into believing that the person recommends a product. In 2023, for instance, actor Tom Hanks alerted fans that an AI-generated replica of him was being used to advertise a dental plan without his permission; such scenarios might give rise to both state ROP and Lanham Act false endorsement claims. State ROP laws can provide broader protection than the Lanham Act, however, as they often prohibit unauthorized commercial uses of NIL regardless of whether they imply any sponsorship or confuse consumers. In addition, some courts have held only individuals with “recognizability” (such as Hanks) may sue for false endorsement, whereas many state laws allow all individuals to sue for ROP violations.CopyrightsCopyrights protect original works of authorship that are “fixed” (i.e., recorded) in a “tangible medium,” including books, paintings, music recordings, and films. The Copyright Act gives copyright owners the “exclusive right” to reproduce (copy), perform, display, and distribute copyrighted works and to make derivative works (adaptations) from them. Generally, a work’s author automatically owns the copyright but may sell or license it to others. In short, whereas ROP laws prohibit unauthorized uses of another person’s identity, copyright law prohibits unauthorized uses of another’s creative works. As one illustration, in January 2024, the estate of comedian George Carlin filed a lawsuit based on an unauthorized comedy program delivered in an AI-generated imitation of Carlin’s voice. The complaint claims defendants infringed their copyrights by making copies of Carlin’s works to train the AI model—similar to other AI-related copyright lawsuits noted in a separate Legal Sidebar—and that defendants violated Carlin’s ROP under California law by using his NIL to promote the comedy program and other media.The ROP intersects with copyright law inasmuch as both fictional and nonfictional copyrighted works often include the NIL of real people, including descriptions, portrayals, recordings, or performances of those people. For example, people depicted in photographs often do not hold the copyright, since the photographer is usually considered the author, but they may have ROP interests implicated by how those photos are used. These rights may come into conflict, such as when a copyright owner displays photographs in a way that commercially exploits the NIL of people shown in the photographs. Section 301 of the Copyright Act provides that the Copyright Act preempts (supersedes) any state law rights that are “equivalent to” a copyright holder’s exclusive rights. Some commentators argue courts have inconsistently applied Section 301 in cases where copyrights conflict with the ROP. Some courts have dismissed lawsuits that allege copyright holders violated the ROP by exercising their exclusive rights under the Copyright Act. For instance, in 2017, one court held that Section 301 preempted a suit claiming the sale of photographs of NCAA athletes violated the athletes’ ROP. Other courts have held that Section 301 does not preempt ROP claims based on advertising. For example, one court held that a sports announcer’s ROP claim was not preempted where an excerpt of his voice from a copyrighted broadcast was used in a commercial for a video game. Congress could clarify the scope of copyright preemption of ROP claims by amending Section 301. In addition, if Congress enacts any new protections for ROP at the federal level, it may specify under what circumstances copyright would preempt such protections.ROP laws can protect commercial interests in live performances, which cannot be copyrighted unless they are “fixed” (e.g., filmed). In the Supreme Court’s only ROP case to date, for instance, a performer sued a television company for broadcasting his “human cannonball” act, undermining ticket sales for the act. The Supreme Court held that the First Amendment did not prevent the performer from asserting a ROP claim against the company. As an exception to the rule that live performances have no copyright protection, in 1994 Congress provided criminal and civil liability for recording and distributing live music performances without permission, although some courts have questioned these laws’ constitutionality. Right of Publicity Concerns Regarding AIRecent advances in generative AI systems, which are trained on large volumes of data to generate new content that may mimic likenesses, voices, or other aspects of real people’s identities, have stimulated congressional interest. Like the above-noted uses of AI to imitate Tom Hanks and George Carlin, the examples below illustrate that some AI uses raise concerns under both ROP laws and myriad other laws. One example of AI’s capability to imitate voices was an AI-generated song called “Heart on My Sleeve,” which sounded like it was sung by the artist Drake and was heard by millions of listeners in 2023. Simulating an artist’s voice in this manner could make one liable under ROP laws, although these laws differ as to whether they cover voice imitations or vocal styles as opposed to the artist’s actual voice. Voice imitations are not, however, prohibited by copyright laws. For example, the alleged copyright violation that caused YouTube to remove “Heart on My Sleeve”—namely, that it sampled another recording without permission—was unrelated to the Drake voice imitation. In August 2023, Google and Universal Music were in discussions to license artists’ melodies and voices for AI-generated songs.The potential for AI to replicate both voices and likenesses was also a point of contention in last year’s negotiations for a collective bargaining agreement between the Screen Actors Guild-American Federation of Television and Radio Artists (SAG-AFTRA)—a union that represents movie, television, and radio actors—and television and movie studios, including streaming services. SAG-AFTRA expressed concern that AI could be used to alter or replace actors’ performances without their permission, such as by using real film recordings to train AI to create “digital replicas” of actors and voice actors. The Memorandum of Agreement between SAG-AFTRA and studios approved in December 2023 requires studios to obtain “clear and conspicuous” consent from an actor or background actor to create or use a digital replica of the actor or to digitally alter the actor’s performance, with certain exceptions. It also requires that the actor’s consent for use of a digital replica or digital alterations be based on a “reasonably specific description” of the intended use or alteration. The agreement provides that consent continues after the actor’s death unless “explicitly limited,” while consent for additional postmortem uses must be obtained from the actor’s authorized representative or—if a representative cannot be identified or located—from the union. In January 2024, SAG-AFTRA announced it had also reached an agreement with a voice technology company regarding voice replicas for video games, while a negotiation to update SAG-AFTRA’s agreement with video game publishers is reportedly ongoing.Commentators have also raised concern with deceptive AI-generated or AI-altered content known as “deepfakes,” including some videos with sexually explicit content and others meant to denigrate public officials. To the extent this content includes real people’s NIL and is used commercially, ROP laws might provide a remedy. Where deepfakes are used to promote products or services—such as the AI replica of Tom Hanks used in a dental plan ad—they may also constitute false endorsement under the Lanham Act. In addition to these laws, some states have enacted laws prohibiting sexually explicit deepfakes, with California and New York giving victims a civil claim and Georgia and Virginia imposing criminal liability. In addition, Section 1309 of the federal Violence Against Women Act Reauthorization Act of 2022 (VAWA 2022) provides a civil claim for nonconsensual disclosure of “intimate visual depictions,” which might be interpreted to prohibit intimate deepfakes—as might some states’ “revenge porn” laws. A bill introduced in the House of Representatives in May 2023, the Preventing Deepfakes of Intimate Images Act, H.R. 3106, would amend VAWA 2022 by creating a separate civil claim for disclosing certain “intimate digital depictions” without the written consent of the depicted individual, as well as providing criminal liability for certain actual or threatened disclosures. Deepfakes may also give rise to liability under state defamation laws where a party uses them to communicate reputation-damaging falsehoods about a person with a requisite degree of fault. Regarding the use of AI in political advertisements, some proposed legislation would prohibit deepfakes or require disclaimers for them in federal campaigns, although such proposals may raise First Amendment concerns. The Protect Elections from Deceptive AI Act, S. 2770 (118th Cong.), for instance, would ban the use of AI to generate materially deceptive content falsely depicting federal candidates in political ads to influence federal elections, while excluding news, commentary, satires, and parodies from liability. Google announced that, as of mid-November 2023, verified election advertisers on its platform “must prominently disclose when their ads contain synthetic content that inauthentically depicts real or realistic-looking people or events.”  Another concern some commentators raise is that AI-generated material might be falsely attributed to real persons without their permission. One writer who focuses on the publishing industry, for instance, found that books apparently generated by AI were being sold under her name on Amazon. Although the company ultimately removed these titles, the writer claimed that her “initial infringement claim with Amazon went nowhere,” since her name was not trademarked and the books did not infringe existing copyrights. As she noted, however, this scenario might give rise to claims under state ROP laws as well as the Lanham Act. In addition, the Federal Trade Commission (FTC) states that “books sold as if authored by humans but in fact reflecting the output of [AI]” violate the FTC Act and may result in civil fines.It is unclear how Section 230 of the Communications Act of 1934 might apply when ROP-infringing content from a third party, including content made with AI, is disseminated through social media and other interactive computer services. Although the law generally bars any lawsuits that would hold online service providers and users liable for third party content, there is an exception allowing lawsuits under “any law pertaining to intellectual property.” Courts differ as to whether state ROP laws and the Lanham Act’s prohibition on false endorsement are laws “pertaining to” IP within the meaning of Section 230. Another Legal Sidebar discusses the application of Section 230 to generative AI more broadly.Considerations for CongressSome commentators have called for federal ROP legislation to provide more uniform and predictable protection for the ROP in the United States. Others have argued that Congress should leave ROP protection to the states on federalism grounds. If Congress decides to craft federal ROP legislation, it might consider the scope of the ROP protections it seeks to enact, the effect of those enactments on state ROP laws, and constitutional authorities and limitations on Congress’s power to enact ROP protections. As noted below, some Members have proposed legislation that would prohibit certain unauthorized uses of digital replicas or depictions of individuals while leaving state ROP laws in place. Scope and Preemptive Effect of Federal ROP LegislationCongress has many options to determine how broadly to protect the ROP via possible federal legislation. Federal ROP legislation might specify, for instance, which aspects of a person’s identity are protected, whether individuals without a commercially valuable identity may assert the ROP, and how long—if at all—the ROP survives a person’s death. Legislation might also specify whether a federal ROP applies broadly to all uses of NIL or only to specific uses. For example, a discussion draft circulated in October 2023 by four Senators, the NO FAKES Act of 2023, would create a civil action for producing, publishing, distributing, or transmitting a “digital replica” of a real person’s image, voice, or likeness without consent, with some exceptions. A bill introduced in the House of Representatives in January 2024, the No AI FRAUD Act, H.R. 6943, would create a civil action for certain conduct involving “digital depictions,” “personalized cloning services,” and “digital voice replicas” of real people without their consent. These proposals differ as to how long postmortem rights may last, among other differences.Additionally, Congress may consider whether federal ROP legislation should preempt or leave in place existing state ROP laws. Existing federal IP laws provide examples of both approaches, as the Patent Act and Copyright Act largely preempt state laws while the Lanham Act and Defend Trade Secrets Act do not. Some commentators argue Congress should preempt state ROP laws to promote greater uniformity and predictability. Alternatively, Congress could create a minimum level of ROP protection under federal law, letting individual states provide greater ROP protections if they wish. The NO FAKES Act of 2023 and No AI FRAUD Act, for instance, state that they do not preempt other ROP protections. Constitutional Authority and Limitations for Federal ROP LegislationWhile Congress has express constitutional authority to enact patent and copyright laws, its power to protect other IP, such as trademarks and trade secrets, derives from its authority to regulate interstate commerce. Some have argued the Commerce Clause of the Constitution gives Congress the power to establish a federal ROP and preempt state ROP laws. ROP laws would likely fall within Congress’s commerce authority so long as they regulate uses of NIL in or substantially affecting interstate commerce.ROP laws create civil liability for certain NIL-containing speech—potentially including media such as books, films, and video games—and therefore raise questions about defendants’ First Amendment rights. Some scholars argue that courts apply inconsistent tests to resolve clashes between the ROP and First Amendment protections. The Ninth Circuit, for example, held that the First Amendment did not shield the use of football players’ avatars in a video game from a ROP lawsuit, since the avatars were not a “transformative use” of the players’ identity under a test first articulated by the Supreme Court of California. The Ninth Circuit reached a different result in a Lanham Act false endorsement case involving another football player’s avatar, holding that the use of the plaintiff’s NIL was “artistically relevant” to the video game and therefore protected under a test developed by the Second Circuit. In a third case, the Ninth Circuit held that the film portrayal of an army veteran was not subject to the transformative use test because, unlike the football players, the veteran had not built up “a marketable performance or identity.” The court therefore applied strict scrutiny, a standard requiring that a law be narrowly tailored to serve a compelling governmental interest. The court held that application of California’s ROP statute would violate the First Amendment because the plaintiff could not demonstrate a “compelling state interest,” given his lack of a marketable identity.Congress might have more latitude to enact ROP laws aimed at commercial speech, such as advertising, which merely proposes a commercial transaction or relates solely to the speaker’s and the audience’s economic interests. Courts typically subject commercial speech regulations to intermediate scrutiny, a less stringent standard than strict scrutiny that requires the government to show that its regulation directly advances a substantial government interest and is not broader than necessary to serve that interest.If Congress enacts federal protections for the ROP, it may seek to mitigate the need for judicial resolution of conflicts between the ROP and the First Amendment, or it may try to give wider latitude to free speech than courts might hold is constitutionally required. To limit the reach of ROP claims, Congress could consider enacting statutory exceptions to ROP liability, possibly similar to the “fair use” defenses in the Copyright Act and the Lanham Act. Congress could also consider limiting federal ROP protections to commercial speech.",https://www.congress.gov/crs_external_products/LSB/PDF/LSB11052/LSB11052.3.pdf
IN12458,2025-02-12T05:00:00Z,2025-06-21T05:11:27Z,Artificial Intelligence: CRS Products,"Advances in artificial intelligence (AI) technologies—including machine learning, generative AI, and facial recognition technologies—are demonstrating many potential benefits across a variety of sectors of the U.S. economy. At the same time, the rapid innovation and proliferation of AI tools have generated concern over their potential disruptive effects, such as generation and dissemination of misinformation, potential shifts in jobs and employment opportunities, and social, ethical, and security risks. Previous Congresses considered a variety of policies and legislation associated with AI technologies and their applications. With interest in AI remaining high, Members of the 119th Congress may continue to engage in AI policymaking. To assist Congress, CRS has produced a range of written and audiovisual products. Below is a selection of CRS products on AI-related policy and legal issues, legislation, and executive branch actions.Selected AI CRS Products AI OverviewCRS Report R48262, Artificial Intelligence: CRS Experts and Points of Contact CRS Video WVB00685, Science and Technology Q&A: Regulating Artificial Intelligence (or, in podcast form, CRS Report WPD00096, Science and Technology Q&A: Regulating Artificial Intelligence) CRS Video WVB00650, Current Issues in Artificial Intelligence CRS Video WVB00615, Artificial Intelligence: Recent Advances and Issues for CongressCRS Report R47644, Artificial Intelligence: Overview, Recent Advances, and Considerations for the 118th Congress CRS Report WPD00050, CRS Science and Technology Podcast: Artificial Intelligence CRS Report R46795, Artificial Intelligence: Background, Selected Issues, and Policy ConsiderationsGenerative AICRS Legal Sidebar LSB11097, Section 230 Immunity and Generative Artificial IntelligenceCRS Legal Sidebar LSB10922, Generative Artificial Intelligence and Copyright LawCRS In Focus IF12426, Generative Artificial Intelligence: Overview, Issues, and Questions for CongressCRS Report R47569, Generative Artificial Intelligence and Data Privacy: A PrimerCRS Video WVB00580, Copyright Law and Generative Artificial IntelligenceCRS Video WVB00554, Science and Technology Q&A: Generative AI and Data PrivacyAI Executive Order 14110 (rescinded January 20, 2025)The following products pertain to Executive Order (E.O.) 14110, “Safe, Secure, and Trustworthy Development and Use of Artificial Intelligence,” which was rescinded on January 20, 2025, by E.O. 14148, “Initial Rescissions of Harmful Executive Orders and Actions.” The original versions of the products below also predate and do not necessarily address E.O. 14179, “Removing Barriers to American Leadership in Artificial Intelligence,” signed January 23, 2025.CRS Report R47843, Highlights of the 2023 Executive Order on Artificial Intelligence for CongressCRS Insight IN12308, The AI Executive Order and Considerations for Federal Privacy Policy CRS Insight IN12289, Law Enforcement Use of Artificial Intelligence and Directives in the 2023 Executive OrderCRS Insight IN12286, The AI Executive Order and Its Potential Implications for DODAI and DefenseSee also AI E.O. 14110 (rescinded January 20, 2025) CRS Report R47599, AUKUS Pillar 2 (Advanced Capabilities): Background and Issues for CongressCRS Report R46458, Emerging Military Technologies: Background and Issues for CongressCRS In Focus IF11105, Defense Primer: Emerging TechnologiesCRS Report R47188, Unmanned Aircraft Systems: Roles, Missions, and Future ConceptsAI and ElectionsCRS Insight IN12222, Artificial Intelligence (AI) and Campaign Finance Policy: Recent Developments CRS In Focus IF12468, Artificial Intelligence (AI) in Federal Election Campaigns: Legal Background and Constitutional Considerations for LegislationAI and FinanceCRS Video WVB00681, Artificial Intelligence in Financial ServicesCRS Report R47997, Artificial Intelligence and Machine Learning in Financial ServicesCRS In Focus IF12399, Automation, Artificial Intelligence, and Machine Learning in Consumer LendingAI and Right of PublicityCRS Video WVB00696, The Right of Publicity and Artificial Intelligence CRS Legal Sidebar LSB11052, Artificial Intelligence Prompts Renewed Consideration of a Federal Right of PublicityFacial RecognitionCRS Report R47541, Immigration: The U.S. Entry-Exit System CRS In Focus IF11333, Deep Fakes and National SecurityCRS Video WVB00328, Law Enforcement Use of Facial Recognition TechnologyCRS Report R46586, Federal Law Enforcement Use of Facial Recognition TechnologyCRS Report R46541, Facial Recognition Technology and Law Enforcement: Select Constitutional ConsiderationsAI and Other SectorsCRS Report R48319, Artificial Intelligence (AI) in Health Care CRS In Focus IF12762, The Macroeconomic Effects of Artificial Intelligence CRS Legal Sidebar LSB11251, Artificial Intelligence and Patent Law CRS In Focus IF11783, Biometric Technologies and Global SecurityCRS Video WVB00708, Disruptive Technology Series: Internet Policy in the Artificial Intelligence EraCRS Video WVB00697, Professional Ethics: Rules of Conduct and Artificial IntelligenceCRS Report R47849, Artificial Intelligence in the Biological Sciences: Uses, Safety, Security, and OversightCRS Video WVB00642, Science and Technology Q&A: Artificial Intelligence in the Biological SciencesCRS In Focus IF12497, Semiconductors and Artificial Intelligence",https://www.congress.gov/crs_external_products/IN/PDF/IN12458/IN12458.1.pdf
IN12222,2024-09-25T04:00:00Z,2025-06-21T04:28:22Z,Artificial Intelligence (AI) and Campaign Finance Policy: Recent Developments,"No federal statute or regulation specifically addresses artificial intelligence (AI) in political campaigns. The Federal Election Campaign Act (FECA) and Federal Election Commission (FEC) regulations govern conduct that calls for election or defeat of federal candidates or solicits funds. They also regulate some advertisements (electioneering communications) that refer to clearly identified federal candidates during preelection periods that do not call for election or defeat. Disclaimer requirements that mandate attribution for communications regulated by campaign finance law appear to apply to ads created with AI. Those requirements do not mandate that such advertising alert the audience, or regulators, to the presence of AI-generated content. Campaign management decisions, such as which technology to use, are generally not subject to regulation.This updated CRS Insight discusses recent developments that could be relevant as Congress monitors or considers legislation related to AI and campaign finance policy. It does not address legal issues. Other CRS products provide information on generative AI and other AI policy areas.AI in Political Campaigns, and Recent Legislative DevelopmentsRecent policy attention to AI in campaigns focuses on “deepfakes,” referring to artificially manipulated audio or video content in political advertising. Such advertising appears to present new challenges for campaigns and voters about how to determine whether communications are authentic. Recent legislation proposes disclaimers, reporting requirements, or prohibitions on deepfakes in federal campaigns or elections. Bills introduced in the 118th Congress include H.R. 3044; H.R. 3106; H.R. 3831; H.R. 4611; H.R. 5586; H.R. 8384; H.R. 8668; H.R. 9639; S. 686; S. 1596; S. 2770; and S. 3875. The Senate Committee on Rules and Administration reported an amended version of S. 3875 on May 15, 2024. The bill would amend FECA to require disclaimers on certain political advertisements that are generated using AI. Legislation (H.R. 1; H.R. 5314) addressing various elections topics, including some provisions concerning deepfakes, passed the House in the 117th Congress but was not enacted.In May 2023, the American Association of Political Consultants (AAPC) issued a statement explaining that its board of directors unanimously “condemn[ed] use of deceptive generative AI content in political campaigns” as inconsistent with the organization’s code of ethics. The AAPC position represents a voluntary professional standard, not a regulatory requirement. The AAPC also stated its support for a February 2024 Federal Communications Commission (FCC) declaratory ruling that calls made with AI-generated voices are “artificial” under the Automated Telephone Consumer Protection Act of 1991 (47 U.S.C. §227), and that using AI-generated voice for robocalls absent prior consumer consent is “illegal.” FCC activity on robocalls is otherwise beyond the scope of this Insight. Despite the focus on AI’s role in political advertising, AI also can serve campaign-management functions. For example, political professionals or volunteers could use AI to automate, or supplement human labor to complete, various internal campaign tasks. According to media reports, campaigns have used AI to perform data analysis, compile opposition research, or draft fundraising appeals.Federal Election Commission Rulemaking ActivityOn June 22, 2023, members of the FEC deadlocked on whether to issue a notice of availability (NOA) to receive comments on an AI rulemaking petition from the interest group Public Citizen. The request asked the FEC to issue rules specifying that the FEC fraudulent misrepresentation of campaign authority prohibition (52 U.S.C. §30124) applied to AI-generated ads. At the June 22 meeting, some commissioners expressed skepticism about the agency’s statutory authority to regulate AI ads; others expressed support for a rulemaking. On July 13, 2023, several Members of Congress wrote to the commission expressing “disappoint[ment]” with the FEC’s action and requested additional information. The commission considered a second Public Citizen rulemaking petition on August 10, 2023. In this case, it approved an NOA. Discussion at the August 10 meeting suggested that at least some commissioners continued to have reservations about the commission’s authority concerning regulating AI ads in particular; about the appropriateness of the FECA fraudulent misrepresentation provision as an avenue to do so; or both. Fifty-two Members of Congress submitted joint comments encouraging the FEC to adopt rules specifying that the fraudulent-misrepresentation provisions apply to ads created using generative AI, and to require disclaimers on ads created with the technology. The matter remained under consideration for the next year.On September 19, 2024, the commission took two actions. It approved a notice of disposition (NOD) to resolve the July 2023 Public Citizen petition without issuing new rules. The NOD notes that the fraudulent-misrepresentation provision is “technology neutral and applies on its face to all means” of fraudulent misrepresentation, “including AI-assisted media.”It approved an interpretive rule, based on existing regulation and statute, confirming that the FECA fraudulent misrepresentation provision also covers communications generated using AI. To summarize, the commission did not issue new rules concerning AI. It clarified that the existing fraudulent misrepresentation provisions in FECA and FEC regulations (11 C.F.R. §110.16(a)) apply to communications generated using AI. Those actions do not affect AI or campaign finance beyond the fraudulent misrepresentation provision, nor do they require additional disclaimers on political advertising. Discussion at the open meeting suggests that a majority of the commission believes that it currently lacks legislative authority to issue broader rules concerning AI.FEC Responses to Federal Communications Commission ActivitySome House and Senate activity has examined a proposed FCC rulemaking that does not directly implicate campaign finance policy and which is largely beyond the scope of this Insight. On July 25, 2024, the FCC approved a notice of proposed rulemaking (NPRM), published in the Federal Register on August 5. If approved, the rules would require certain licensees (e.g., broadcasters) to (1) announce on air that a political ad contains “AI-generated content”; and (2) include the information in their “political files” of advertising contracts. Another CRS product discusses identification requirements on political advertising in telecommunications law and regulation. On June 3, 2024, FEC Chair Sean Cooksey wrote to FCC Chair Jessica Rosenworcel, stating that the reportedly forthcoming FCC proposed rules would infringe on FEC jurisdiction and could cause confusion before the general election. Three days later, FEC Vice Chair Ellen Weintraub wrote to Rosenworcel stating that the FCC could add telecommunications expertise to AI regulation. It is unclear how or whether the FEC might respond if the FCC adopted the proposed rules.Potential Policy Considerations for CongressIf pursuing legislation, Congress might need to determine whether to do so narrowly, such as by addressing specific AI issues, or to also address other campaign finance or elections topics. Congress has pursued both approaches to campaign finance regulation recently. If Congress chose to task the FEC with pursuing rulemaking without also providing additional statutory guidance, it is possible that the commission would be unable to agree, with the four of six minimum required votes, about how to proceed. Maintaining the status quo likely would reinforce the emerging debate about whether additional regulation is needed, including about what role industry should play. Congress could also require agency (or committee or task force) study of AI issues before, or in addition to, other policymaking. Amending FECA would be a typical approach to further regulate ads that are made by political committees; or that solicit funds, engage in express advocacy, or refer to federal candidates through electioneering communications. Although Congress could also amend FECA or another statute to require disclaimers on ads that do not meet those requirements (e.g., issue advocacy), federal campaign finance law currently generally does not regulate issue advocacy. As noted above, amending telecommunications law or regulation could affect broadcasters or other entities that transmit ads, and could affect issue advocacy in ways that campaign finance law and regulation do not. Prohibiting AI-generated ads or requiring additional disclaimers might raise First Amendment concerns, such as those discussed in another CRS campaign finance product.",https://www.congress.gov/crs_external_products/IN/PDF/IN12222/IN12222.5.pdf
IG10077,2025-04-25T04:00:00Z,2025-06-21T06:38:06Z,Artificial Intelligence (AI) Taxonomy,"/Information as of April 25, 2025. Prepared by Laurie Harris, Analyst in Science and Technology Policy; Nora Wells, Analyst in Health Policy; and Juan Pablo Madrid, Visual Information Specialist. Experts generally describe AI as the broad concept of machine-based systems that can do tasks commonly thought to require human intelligence, like making predictions and recommendations, translating languages, or generating text, images, audio, and video. The term has evolved as research and applications of AI technologies have advanced, leading to the development of new terminology. As Congress works to develop and enact legislation related to AI technologies, questions frequently arise around what terms to define, how to define them, and how they are interrelated. This infographic describes key AI terms and illustrates how they are related to one another. (Note that while this represents a synthesis of ideas from many AI experts and stakeholders, the definitions and intersections of these terms are evolving and still under debate. There are not universally agreed-upon bright lines or hierarchies among terms.) There has been some debate over whether expert systems should still be considered AI, as they cannot adapt to unexpected inputs or variables outside of what they were trained on (i.e., they lack a learning component).Artificial Intelligence (AI) is a broad term referring to algorithms and techniques that aim to give computer systems the ability to learn new concepts or tasks and solve complex problems in a manner that mimics human intelligence. The concept of AI and AI systems can encompass a range of technologies, methodologies, and application areas, such as natural language processing, facial recognition, and robotics.Expert Systems, an early approach to making machines that mimic human intelligence, are algorithms encoded with expert knowledge but lacking a learning component. In these rules-based systems, programmers solve a problem, then program routines and rules that the system uses to respond to new inputs.Robotics refers to the design, construction, and use of machines (robots) to replicate, or assist with, human actions. Robotics is one area where AI can be applied in physical applications, sometimes called robotic learning, to help robots learn and improve their performance through self-exploration or guidance from human operators.Machine Learning (ML), often referred to as a subfield of AI, uses algorithms to enable systems to identify and learn from patterns or relationships in data without being explicitly programmed. The performance of these systems can improve as they learn from more data to then make predictions or decisions on new, unseen data.Deep Learning (DL), a subset of ML, uses neural network (NN) techniques to process and analyze large-scale, complex data, including text, image, and audio. NNs were originally inspired by how layers of neurons in the human brain signal each other, with the artificial neurons grouped into layers of interconnected nodes (i.e., computational units). NNs “learn” by adjusting the connections between nodes, tuning the system based on characteristics of the training data to correspond to the correct output; this is the general process of AI model training. A DL system usually has numerous layers that can consist of thousands or millions of processing nodes. Given the size and complexity of most NNs built for AI model training, the terms DL and NN are often used interchangeably.Natural Language Processing (NLP) refers to the use of rules-based or ML approaches to understand the structure and meaning of written or spoken human language. DL, GenAI, and LLMs have been leveraged for NLP applications such as improving language translation and powering chatbots, thus spanning multiple AI terms and approaches.Generative AI (GenAI) refers to AI systems that can generate content—such as written material, audio, images, or computer code—from prompts using advanced techniques such as NNs that help the underlying models better understand how data elements influence and depend on one another. General-Purpose AI Models, often referred to as foundation models (FMs), are GenAI models trained on large amounts of diverse types of data that can be fine-tuned for a wide range of downstream tasks. Large language models (LLMs) are one category of FMs designed to learn from and generate text for applications such as language translation, question answering, and  computer code generation.",https://www.congress.gov/crs_external_products/IG/PDF/IG10077/IG10077.1.pdf
R48555,2025-06-04T04:00:00Z,2025-08-01T11:38:01Z,Regulating Artificial Intelligence: U.S. and International Approaches and Considerations for Congress,"Artificial intelligence (AI) presents many potential benefits and challenges in the private and public sectors. No federal legislation establishing broad regulatory authorities for the development or use of AI or prohibitions on AI has been enacted. Recent Congresses have passed primarily more targeted AI provisions. Different Administrations have focused their attention on federal engagement in AI, albeit with somewhat different emphases on specific topics. The focus on AI safety under the Biden Administration appears to be shifting toward security concerns during the second Trump Administration. Stakeholders in the United States have debated how to approach AI innovation and regulation in order to harness the opportunities of AI technologies, such as enhanced government operations and worker efficiency, while minimizing potential problems, such as bias and inaccuracies in AI-generated output. U.S. AI Governance and RegulationOutside of broad AI governance frameworks, most of the U.S. regulatory efforts regarding AI have centered on (1) federal agency assessments and enforcement of existing regulatory authorities, (2) exploration of whether individual agencies require additional authorities, and (3) securing voluntary commitments from industry. Much of the legislation proposed in the 118th and 119th Congresses have emphasized the development of voluntary guidelines and best practices and reporting of industry-conducted evaluations of AI systems. The approach of the U.S. federal government as a whole appears to be cautious in regard to regulating AI in the private sector and more focused on oversight of federal government uses of AI. In the absence of federal AI regulations, states have been enacting their own laws. Critics assert that such a patchwork of AI laws creates challenges for companies and that a nationwide regulatory structure may incentivize product development. Approaches to AI Governance and Regulation, Including International ApproachesProponents of broad federal AI regulations assert that they would lead to less legal uncertainty for AI developers and improve the public’s trust in AI systems, thus supporting AI innovation. Opponents of broad federal AI regulations assert that industry is taking steps to self-regulate and that additional regulation would stifle innovation at a time when international competition in AI is accelerating, which could lead to negative economic and national security outcomes for the United States. Other analysts have criticized such characterizations as presenting a false dichotomy between regulation and innovation and instead support a mixture of targeted, flexible approaches depending on the AI technology and its application. Similar to the United States, some other countries, including the United Kingdom, have taken to date a measured approach to regulating AI. In contrast, the European Union (EU) has enacted a broad regulatory approach through the EU AI Act, which classifies AI systems into risk categories with different degrees of requirements and obligations. Some analysts have raised concerns that the EU AI Act creates or will create barriers for companies developing and deploying AI. China has enacted targeted AI laws and is working on broader AI regulation, though China’s economic and science and technology policies feature a heavy government role in private sector development. China’s approach has been characterized as a vertical, technology-specific framework influenced by national security concerns and economic development goals, with the EU’s AI Act described as a horizontal, risk-based framework focused on ethical considerations and transparency.Considerations and Options for CongressCongress and the Administration might frame AI legislation and policies in various ways. One approach could include actions aimed at promoting AI safety and averting risks to people, another approach may focus more on security, and yet another approach may focus on accelerating innovation potentially accompanied by voluntary commitments from industry. These approaches, among others, may also be combined. Congressional actions might focus on leveraging federal agencies’ existing authorities without enacting additional AI-specific laws or on creating new cross-sector authorities or broad regulations to address potential risks from AI, such as transparency and accountability requirements. Additionally, congressional actions might focus on providing federal agencies with authorities or direction to support domestic AI development—such as through public-private partnerships and providing resources for AI research and education—or engaging with international efforts to harmonize AI governance. In a time of rapid AI development, such efforts may need to frequently evolve or incorporate mechanisms for periodic review and flexibility at the state and federal levels.",https://www.congress.gov/crs_external_products/R/PDF/R48555/R48555.4.pdf
LSB10922,2025-06-16T04:00:00Z,2025-07-18T17:14:55Z,Generative Artificial Intelligence and Copyright Law,"Innovations in artificial intelligence (AI) have raised several new questions in the field of copyright law. Generative AI programs—such as Open AI’s DALL-E and ChatGPT programs, Stability AI’s Stable Diffusion program, and Midjourney’s self-titled program—are able to generate new images, texts, and other content (or “outputs”) in response to a user’s textual or other prompts. Generative AI programs are trained to create such outputs partly by exposing them to large quantities of existing writings, photos, paintings, or other works.This Legal Sidebar explores questions that courts and the U.S. Copyright Office have confronted regarding whether generative AI outputs may be copyrighted as well as whether training and using generative AI programs may infringe copyrights in other works. Other CRS Legal Sidebars explore questions AI raises in the intellectual property fields of patents and the right of publicity.Copyright in Works Created with Generative AIDo Copyrighted Works Require a Human Author?The question of whether copyright protection may be afforded to AI outputs—such as images created by Midjourney or texts created by ChatGPT—hinges largely on the legal concept of “authorship.” Article I, Section 8, Clause 8 of the U.S. Constitution, often referred to as the Intellectual Property (IP) Clause, empowers Congress to “secur[e] for limited Times to Authors . . . the exclusive Right to their . . . Writings.” Based on this authority, the Copyright Act affords copyright protection to “original works of authorship.” While the Constitution and Copyright Act do not explicitly define who (or what) may be an “author,” U.S. courts to date have not recognized copyright in works that lack a human author—including works created autonomously by AI systems.Before the proliferation of generative AI, U.S. courts did not extend copyright protection to various nonhuman authors, holding that a monkey who took photos of himself lacked standing to sue under the Copyright Act; that human authorship was required to copyright a book purportedly inspired by celestial beings; and that a living garden could not be copyrighted. The U.S. Copyright Office has also long maintained that copyrighted works must be “created by a human being” and therefore refused to register works that are “produced by a machine or mere mechanical process that operates randomly or automatically without any creative input or intervention from a human author.”At least one lawsuit has—unsuccessfully, thus far—challenged the human-authorship requirement in the context of AI. In June 2022, Stephen Thaler sued the Copyright Office for denying his application to register a visual artwork that he claims was authored “autonomously” by an AI program. Dr. Thaler argued that human authorship is not required by the Copyright Act. In August 2023, a U.S. district court granted summary judgment in favor of the Copyright Office. The court held that “human authorship is an essential part of a valid copyright claim,” reasoning that only human authors need copyright as an incentive to create expressive works. In March 2025, the U.S. Court of Appeals for the D.C. Circuit affirmed the district court’s decision in Thaler v. Perlmutter, holding that the Copyright Act “requires all eligible work to be authored in the first instance by a human being.” The court reasoned that several provisions of the Copyright Act imply that it uses the word “author” only to refer to human beings, including provisions (1) vesting copyright ownership “initially in the author”; (2) limiting copyright duration to 70 years after “the author’s death”; (3) providing for inheritance of certain rights by the author’s “widow or widower” or “surviving children or grandchildren”; (4) requiring a signature to transfer copyright ownership; (5) extending protection to unpublished works regardless of the author’s “nationality or domicile”; and (6) defining a “joint work” based on the authors’ “intention” to merge their contributions in a certain way. In addition, the D.C. Circuit observed that the Copyright Office had adopted the human-authorship requirement before Congress enacted the current Copyright Act. The court thus inferred that Congress meant to adopt the human-authorship requirement when it enacted the law. Based on its holding that the Copyright Act requires human authorship, the court found it unnecessary to consider the Copyright Office’s argument that the Constitution’s IP Clause also requires human authorship. On May 12, 2025, the court denied Dr. Thaler’s petition to rehear the case en banc (i.e., by all of the court’s judges). May Humans Copyright Works That They Create Using AI?Assuming that copyrightable works require a human author, works created by humans with the assistance of generative AI might be entitled to copyright protection depending on the nature of human involvement in the creative process. As discussed below, the Copyright Office has sought to delineate what authors must do to satisfy the human-authorship requirement when using generative AI.In March 2023, the Copyright Office released Copyright Registration Guidance regarding “works containing material generated by [AI]” (the AI Guidance). Granting that human authors may use AI in the creative process, the AI Guidance states that “what matters is the extent to which the human had creative control over the work’s expression.” Thus, the AI Guidance states, when AI “determines the expressive elements of its output, the generated material is not the product of human authorship.” On the other hand, works containing AI-generated material may be copyrighted under some circumstances, such as “sufficiently creative” human arrangements or modifications of AI-generated material or works that combine AI-generated and human-authored material. The AI Guidance states that authors may claim copyright protection only “for their own contributions” to such works, and they must identify and disclaim AI-generated parts of the works when applying to register their copyright. Three copyright registration denials highlighted by the Copyright Office illustrate that, in general, the office will not find human authorship where an AI program generates works in response to user prompts:Zarya of the Dawn: A February 2023 decision that AI-generated illustrations for a graphic novel were not copyrightable, although the human-authored text of the novel and overall selection and arrangement of the images and text in the novel could be copyrighted.Théâtre D’opéra Spatial: A September 2023 decision that an artwork generated by AI and then modified by the applicant could not be copyrighted, since the applicant failed to identify and disclaim the AI-generated portions as required by the AI Guidance.SURYAST: A December 2023 decision that an artwork generated by an AI system combining a “base image” (an original photo taken by the applicant) and a “style image” the applicant selected (Vincent van Gogh’s The Starry Night) could not be copyrighted, since the AI system was “responsible for determining how to interpolate [i.e., combine] the base and style images.”While the Copyright Office’s decisions indicate that it may not be possible to obtain copyright protection for many AI-generated works, the issue remains unsettled. An applicant may file suit in U.S. district court to challenge the Copyright Office’s final decision to refuse to register a copyright. The putative author of Théâtre D’opéra Spatial, for instance, has sued the Copyright Office for declining to register that work. While the Copyright Office notes that courts sometimes give weight to the office’s experience and expertise, courts are not legally bound to adopt the office’s interpretations of the Copyright Act, such as its application of the authorship requirement to AI-assisted works. In January 2025, the Copyright Office published the part of its Copyright and Artificial Intelligence report addressing the copyrightability of AI-generated works. Reinforcing the AI Guidance’s emphasis on “creative control,” the report concludes that, “given current generally available technology, prompts alone do not provide sufficient human control to make users of an AI system the authors of the output.” The report contends that the Copyright Act’s distinction between copyrightable “works” and noncopyrightable “ideas” precludes copyrightability for works generated by AI in response to user prompts. Specifically, the report argues, “[p]rompts essentially function as instructions that convey unprotectible ideas” and “do not control how the AI system processes them in generating the output.” Some commentators assert that certain AI-generated works should receive copyright protection, comparing AI programs to other tools human authors have used to create copyrighted works. For example, the U.S. Supreme Court held in the 1884 case Burrow-Giles Lithographic Co. v. Sarony that photographs can be entitled to copyright protection where the photographer makes decisions regarding creative elements such as composition, arrangement, and lighting. Some copyright applicants argue that generative AI programs may function as tools, analogous to cameras. The Copyright Office disputes the photography analogy, arguing that AI users do not exercise sufficient control to characterize generative AI as a tool used by an author. Instead, the Copyright Office has analogized an AI user to “a client who hires an artist” and gives that artist only “general directions.” At least one applicant’s attorney has argued that the Copyright Act does not require such exacting creative control, observing that certain photographs and modern art incorporate a degree of happenstance.Regarding works that combine human-authored and AI-generated material, the Copyright Office reports that, in the time since it issued the AI Guidance, it “has registered hundreds of works that incorporate AI-generated material, with the registration covering the human author’s contribution to the work.” The office contends that new legislation regarding “the copyrightability of AI-generated material” is currently not needed, indicating that courts “will provide further guidance on the human authorship requirement as it applies to specific uses of AI” and that, since each work must be analyzed individually, “greater clarity would be difficult to achieve” through legislation.Copyright Infringement by Generative AI ProgramsDoes the AI Training Process Infringe Copyrights in Other Works?AI systems are trained to create literary, visual, and other works by exposure to large amounts of data, which may include text, images, and other works downloaded from the internet or otherwise obtained by AI companies. This process often involves making digital copies of existing works. Copyright owners have filed several dozen lawsuits claiming that creating these digital copies without permission to train AI systems infringes their exclusive right to make reproductions (or copies). Many AI companies and some legal scholars argue that using copyrighted works to train AI systems constitutes fair use and is therefore noninfringing. Whether or not unauthorized copying constitutes fair use depends on four nonexclusive factors that Congress set forth in the Copyright Act: the purpose and character of the use, including whether such use is of a commercial nature or is for nonprofit educational purposes;the nature of the copyrighted work;the amount and substantiality of the portion used in relation to the copyrighted work as a whole; andthe effect of the use upon the potential market for or value of the copyrighted work.The Supreme Court has stated that fair use is a “flexible” doctrine, and “its application may well vary depending upon context.” As to the first factor, the Court has held that some uses with a “transformative” purpose (such as parodies) may be fair, although in 2023 it cautioned that transformativeness is “a matter of degree.” The Court has described the fourth factor as the “most important” one. In June 2025, the U.S. District Court for the Northern District of California issued summary judgment decisions in two lawsuits concerning generative AI and fair use. In one case, Bartz v. Anthropic PBC, the defendant had trained an AI system (Claude) on books obtained through a combination of buying print copies and downloading digital copies from “pirate sites.” The district court held that it was fair use to copy the books to train Claude, reasoning that generative AI is “quintessentially transformative” (factor 1), that copying entire books was “reasonably necessary” to train Claude (factor 3), and that Claude does not create copyright-infringing outputs that would “displace demand” for the books (factor 4). The court found that only factor 2 tilted against fair use, since the books were used for their “expressive qualities.” The court also held that it was fair use for Anthropic to convert the print books it bought into digital versions. But, the court held, it was not fair use for Anthropic to download pirated books and maintain them in a “central library,” only parts of which were used for AI training. On July 17, the court certified a class action for copyright owners of books Anthropic had downloaded from certain pirate libraries.In the other case, Kadrey v. Meta Platforms, Inc., a different judge ruled that, based on the record before the court, it was fair use for Meta to train a generative AI system (Llama) on books it downloaded from pirated “shadow libraries.” As in Bartz, the court in Kadrey held that this use was “highly transformative” and that factor 1 as well as factor 3 favored fair use, while factor 2 did not. Regarding factor 4, the court criticized plaintiffs for failing to develop evidence of market harm, potentially forfeiting a winning argument. The court opined in dicta that factor 4 might weigh decisively against fair use in other generative AI cases. Disagreeing with the judge in Bartz, the court in Kadrey reasoned that market harms from generative AI are not limited to copyright-infringing outputs but may also include noninfringing outputs that compete with copyrighted works (for example, because they concern the same topic). The Bartz and Kadrey judges’ disagreement reflects a wider debate over whether “market dilution” from noninfringing AI outputs is a cognizable harm under factor 4. The Copyright Office, for instance, has argued that market dilution may weigh against fair use, while some scholars have criticized this view. The Bartz and Kadrey decisions also took different approaches to the initial downloading of plaintiffs’ books without the authors’ permission. Unlike in Bartz, the court in Kadrey did not conduct a separate fair-use analysis of these downloads. The court reasoned that Meta’s purpose in downloading the books was to train Llama, and “[b]ecause Meta’s ultimate use of the plaintiffs’ books was transformative, so too was Meta’s downloading . . . .” By contrast, the court in Bartz found that Anthropic had impermissibly obtained “a central library of works to be available for any number of further uses.” Yet in dicta, the Bartz court doubted that it would ever be fair use to download books from pirate sites instead of purchasing them. On July 14, Anthropic filed a motion seeking an appeal or reconsideration of the court’s decision in Bartz, arguing that the decisions in Bartz and Kadrey “cannot be reconciled” on the downloading issue.Given the fact-specific nature of fair use, courts in other generative AI cases may conduct their own analyses and reach different answers as to fair use and its constituent factors. In May 2025, the Copyright Office released a prepublication version of the Generative AI Training part of its Copyright and Artificial Intelligence report, which concluded that “it is not possible to prejudge litigation outcomes” and that “some uses of copyrighted works for generative AI training will qualify as fair use, and some will not.” Do AI Outputs Infringe Copyrights in Other Works?Some outputs of AI programs might infringe copyrights in other works they resemble that were used to train the AI. Copyright owners may be able to establish that such outputs infringe their copyrights if the AI program both (1) had access to their works and (2) created “substantially similar” outputs. First, to establish copyright infringement, a plaintiff must prove the infringer “actually copied” the underlying work. This element is sometimes proven circumstantially by evidence that the infringer “had access to the work.” For AI outputs, access might be shown by evidence that the AI program was trained using the underlying work. Such evidence might show, for instance, that a copy of the underlying work was located on an internet site that was downloaded or “scraped” to train the AI program. Second, a plaintiff must prove that the new work is “substantially similar” to the copyrighted work to establish infringement. The substantial similarity test is difficult to define. Courts have variously described the test as requiring, for example, that the works have “a substantially similar total concept and feel” or “overall look and feel” or that “the ordinary reasonable person would fail to differentiate between the two works.” Leading cases have also stated that this determination considers both “the qualitative and quantitative significance of the copied portion in relation to the plaintiff’s work as a whole.” For AI-generated outputs, no less than for traditional works, the “substantial similarity” analysis may require courts to make these kinds of comparisons between the output and the plaintiff’s work. OpenAI has argued that “[w]ell-constructed AI systems generally do not regenerate, in any nontrivial portion, unaltered data from any particular work in their training corpus.” Thus, according to OpenAI, copyright-infringing outputs would be “an unlikely accidental outcome” of such systems. One study found “a significant amount of copying” in less than 2% of the images created by Stable Diffusion, though the authors claimed that their methodology “likely underestimates the true rate” of copying.Two kinds of AI outputs may raise special concerns. First, some AI programs may be used to create works involving existing fictional characters. These works may run a heightened risk of infringement, since characters sometimes enjoy copyright protection distinct from the specific works in which they appear. Second, some AI programs may be prompted to create works “in the style of” a particular artist or author, although some AI programs may now be designed to “decline” such prompts. These outputs are not necessarily infringing, as copyright law generally protects only against the copying of specific works. For example, a song generated by AI in the style and simulated voice of a human performer might not infringe any copyright, although voice simulations may potentially violate some state right-of-publicity laws. As a separate issue, judges and commentators disagree about whether market dilution from stylistically similar outputs could weigh against a fair-use defense for training AI on an author’s work, as discussed above.If a generative AI output infringes a copyright in an existing work, both the AI user and the AI company could potentially be liable under current law. For instance, the user might be directly liable for prompting the AI program to generate an infringing output. It may be challenging to analyze the user’s liability in some cases, since the user might not have direct access to—or even be aware of—a copyrighted work purportedly infringed by an AI output. The AI company could also potentially face liability under the doctrine of “vicarious infringement.” Vicarious infringement applies to defendants who have “the right and ability to supervise the infringing activity” and “a direct financial interest in such activities.”Considerations for CongressCongress may consider whether to address any of the copyright law questions raised by generative AI programs through amendments to the Copyright Act or other legislation. Congress may, for example, consider legislation clarifying whether AI-generated works are copyrightable or under what circumstances the process of training generative AI programs may constitute fair use. Alternatively, given the limited time courts have had to address these issues, Congress may adopt a wait-and-see approach. As courts decide more cases involving generative AI, they may be able to provide greater guidance and predictability in this area. Based on such outcomes, Congress may reassess whether legislation is needed.Congress may also consider the practical implications of requiring AI companies to identify, seek permission from, or compensate copyright owners should court decisions or future legislation determine that training generative AI systems is not a fair use of copyrighted works. Commentators have debated whether it is feasible to require companies to identify and pay owners of the large number of works needed to train AI systems, as well as whether the value of such compensation to owners would be outweighed by transaction or administration costs. One scholar, acknowledging that “[i]t would . . . be impossible for an AI developer to identify and clear billions of rights claims on an individual basis,” argues that it may be feasible instead to create markets for AI training data via means such as content aggregation (e.g., TV streaming services), collective management organizations (or CMOs, such as those that manage rights to musical works), compulsory licenses (which exist for certain uses of sound recordings), or technological measures (e.g., giving rightsholders a means to opt out of using works for AI training). Another scholar questions the practicality of CMOs for AI training data, in part due to the large volume and variety of works used to train AI, while observing that some “large rights holders” (such as the Associated Press) have individually contracted with AI companies for the use of their works.In the Generative AI Training part of its Copyright and Artificial Intelligence report, the Copyright Office contends that voluntary licensing—either by individual rightsholders or through CMOs—is sometimes (though perhaps not always) feasible for licensing copyrighted works to train AI. For instance, the report indicates that “[s]ome AI systems have now been trained exclusively on licensed or public domain works.” The report expresses normative and practical reservations about nonvoluntary approaches to licensing training data, such as compulsory licenses or requiring copyright holders to “opt out” if they do not consent to the use of their works to train AI. Thus, the report “recommends allowing the licensing market to continue to develop without government intervention” for now. Congress may consider these recommendations as well as differing perspectives such as those surveyed in the report.",https://www.congress.gov/crs_external_products/LSB/PDF/LSB10922/LSB10922.10.pdf
IF12426,2025-04-02T04:00:00Z,2025-06-21T06:27:01Z,"Generative Artificial Intelligence: Overview, Issues, and Considerations for Congress","Generative artificial intelligence (GenAI) refers to AI models, in particular those that use machine learning (ML) and are trained on large volumes of data, that are able to generate new content. In contrast, other AI models may have a primary goal of classifying data, such as facial recognition image data, or making decisions, such as those used in automated vehicles. GenAI, when prompted (often by a user inputting text), can create various outputs, including text, images, videos, computer code, or music.The public release of many GenAI tools, and the race by companies to develop ever-more powerful AI models, have generated widespread discussion of their capabilities, potential concerns with their use, and debates about their governance and regulation. This CRS In Focus describes the development and uses of GenAI, concerns raised by the use of GenAI tools, and considerations for Congress. BackgroundAI can generally be thought of as computerized systems that work and react in ways commonly considered to require human intelligence, such as learning, solving problems, and achieving goals under uncertain and varying conditions, with varying levels of autonomy. AI can encompass a range of technologies, methodologies, and application areas, such as natural language processing, robotics, and facial recognition.The AI technologies underpinning many GenAI tools are the result of decades of research. For example, recurrent neural networks (RNNs), a type of ML loosely modeled after the human brain that detects patterns in sequential data, underwent much development and improvement in the 1980s-1990s. RNNs can generate text, but they have limited ability to retain contextual information across large strings of words, are slow to train, and are not easily scaled up by increasing computational power or training data size.More recent technical advances—notably the introduction of the Transformer architecture by Google researchers in 2017 and improvements in generative pre-trained transformer (GPT) models since around 2019—have contributed to dramatic improvement in GenAI performance. Transformer models process a sequence of whole sentences rather than analyzing word by word. They use mathematical techniques called attention or self-attention to detect how data elements, even when far away sequentially, influence and depend on each other. These methods make GPT models faster to train, more efficient in understanding context, and highly scalable. Other critical components to recent GenAI advances have been the availability of large amounts of data and the size of their language models. Large language models (LLMs) are AI systems that aim to model language, sometimes using millions or billions of parameters (i.e., numbers in the model that determine how inputs are converted to outputs). Repeatedly tweaking these parameters, using mathematical optimization techniques and large amounts of data and computational power, increases model performance. Notably, GenAI models work to match the style and appearance of the underlying training data. They have also demonstrated emergent abilities, meaning capabilities that their developers and users did not anticipate but that are emerging as the models grow larger.LLMs have been characterized as foundation models (also called general-purpose AI), meaning models trained on large and diverse datasets that can be adapted to a wide range of downstream tasks. As described by the Stanford University Institute for Human-Centered AI, foundation models may be built on or integrated into multiple AI systems across various domains (e.g., text-based GPT models that can perform arithmetic and computer programming tasks, which were outside the scope of their original training). This capability has the potential for both benefits (e.g., concentrating efforts to reduce bias and improve robustness) and drawbacks (e.g., security failures or inequities that flow to downstream applications).Capabilities and AdvancesThe increase in size of recent GenAI models (with hundreds of billions or trillions of parameters) has led to improved capabilities over previous systems (with millions or a few billion parameters). According to the AI Index 2024 Annual Report, “LLMs have surpassed human performance on traditional English-language benchmarks,” and the report argues that the “rapid advancement has led to the need for more comprehensive benchmarks.”Initial GenAI tools tended to excel at single input and output types—such as text to text for chatbots or text to image for image generators. More multimodal GenAI models are available now, meaning they can process and integrate information from multiple types of data or modalities simultaneously. For example, Google’s Gemini models can use text-image-audio-video inputs and provide text-image outputs.Beginning in late 2024, companies introduced what have been termed reasoning models—models that use a chain-of-thought technique in an attempt to “refine their thinking process, try different strategies, and recognize their mistakes” (e.g., OpenAI’s o1 and o3-mini models, Anthropic’s Claude 3.7 Sonnet, and DeepSeek’s R1 model), though reasoning models still frequently make mistakes. Along with the development of large-scale models, innovations in model design and training have led to improvements in the speed and capabilities of GenAI tools.Concerns and Potential RisksDespite the impressive abilities of GenAI, its rapid growth has raised concerns and discussions about managing potential risks. For example, the tendency of GenAI to produce incorrect or misleading results, sometimes referred to as confabulation or hallucinating, might lead to the tools generating and amplifying misinformation or being used to create and spread disinformation. For example, in January 2024, researchers found that legal mistakes with LLMs were “pervasive,” and in December 2024, expert testimony was thrown out after a judge discovered fake information produced by GenAI. Because the models are generally trained on large amounts of data scraped from the internet, they can incorporate, reflect, and potentially amplify biases in such data. OpenAI has noted that even powerful models like GPT-4 have limitations, being “not fully reliable,” and that “great care should be taken when using language model outputs, particularly in high-stakes contexts.” Concerns about model inputs and limitations have led to calls for greater transparency about model training and validation.Safety and security risks are ongoing concerns with GenAI tools. For example, jailbreaks can trick an AI system into ignoring built-in safety rules. Prompt injection attacks disguise malicious inputs as non-malicious prompts, manipulating a system into such actions as leaking sensitive data. Also, depending on a company’s privacy policies, U.S. user data may be stored on servers in foreign countries as has been flagged with the Chinese company DeepSeek. As GenAI use grows, analysts have been considering how it might affect jobs and productivity. For example, will these tools complement workers’ skills in existing jobs, create new jobs, or automate some jobs, displacing workers? What impact might GenAI have on economic productivity? While these have been long-standing questions for automation and AI technologies, the speed, capability, and adoption of GenAI has heightened labor-related concerns.Federal AI Laws and GenAI LegislationBills focused on AI, or including AI-focused provisions, have been enacted in prior Congresses. For example, the National Artificial Intelligence Initiative Act of 2020 (Division E of P.L. 116-283) codified the establishment of a national AI initiative and associated federal offices and committees. Specifically regarding GenAI, the Identifying Outputs of Generative Adversarial Networks (IOGAN) Act (P.L. 116-258) directed federal support of research on generative adversarial networks. Some Members of Congress have introduced bills that included GenAI, such as those pertaining to the development of technical standards, deepfakes and AI-generated voice messages, and transparency and accountability of GenAI use.Considerations for CongressFederal agencies and Congress have been exploring GenAI uses, including for office tasks such as creating and summarizing content, writing speeches, and drafting bills. At the same time, several bills have been introduced in the 119th Congress regarding GenAI, building on various bills introduced in the 118th Congress to implement guardrails for GenAI technologies in the private sector. As GenAI development continues and its use grows, Congress might consider questions and potential actions such as theseBias and ethics of use. The private and public sectors may be using federal guidance and frameworks for AI to address bias and manage risks for GenAI. Congress might consider whether additional sector-specific guidance would be helpful and whether the deployment of GenAI models in high-risk scenarios (e.g., mental health therapy or generating forensic sketches) requires restrictions.Testing and transparency. Many of the biggest models deployed today are closed-source and proprietary. Companies state that they conduct internal testing and are evaluating options for external validation and testing. Congress might consider the adequacy of industry self-assessment and whether and how to support or require independent testing and reporting of results.Economic and workforce impacts. Researchers in industry and academia have begun analyzing GenAI’s potential widespread effects on labor. In November 2024, the National Academy of Sciences released an updated study on AI and the future of work. Congress might consider the appropriate federal role in supporting U.S. workforce reskilling or upskilling in response to shifting job tasks caused by the implementation of GenAI. It might also consider whether and how to increase AI expertise in the government’s own workforce, such as using statutory authority to establish a federal AI scholarship-for-service program (per P.L. 117-167).Research and competition. Estimates have put training costs for large GenAI models in the millions to tens of millions of dollars. Some analysts have argued that cost, use of proprietary data, and access to vast computing power will create a divide between those who can train the most cutting-edge LLMs (e.g., large technology firms) and those who cannot (e.g., nonprofits, start-ups, universities). Congress might consider whether to support access to data, training, and computing resources, such as through codifying an effort like the National AI Research Resource. Further, the release of advanced, open-source models such as DeepSeek’s R1 have raised concerns about national security and international competition. Congress might consider whether to support U.S. leadership in AI innovation, such as through technical standards development and coordination with allied countries.Oversight and regulation. In considering possible regulation of GenAI technologies, Congress might weigh potential impacts on innovation and international competitiveness. Do federal regulatory agencies have the authorities and resources to adequately oversee GenAI tools to minimize risks and support benefits? If not, what additional authorities are needed? How might federal oversight and regulation for GenAI be distinct from that for AI technologies more broadly?",https://www.congress.gov/crs_external_products/IF/PDF/IF12426/IF12426.4.pdf
IF12762,2025-04-01T04:00:00Z,2025-06-21T06:29:06Z,The Macroeconomic Effects of Artificial Intelligence,"While various forms of artificial intelligence (AI) technologies have existed and been used for decades, the recent popularization of AI products such as ChatGPT have spurred further research and debate about how these technologies could impact the economy. AI potentially has wide-ranging uses in the production of goods and services, which could affect the macroeconomy through the labor market, productivity growth, and economic growth, to name a few. However, whether AI will prove to be as economically transformational as some suggest remains to be seen and will depend on a number of complex factors. Some Members of Congress are increasingly interested in AI, including its economic impacts. For example, the House announced a bipartisan task force on AI in February 2024. What Is Artificial Intelligence?AI is a broad term referring to algorithms and techniques that aim to give computer systems the ability to learn new concepts or tasks and to reason and solve complex problems in a manner that mimics human intelligence. Machine learning (ML) is generally considered a subfield of AI focused on developing systems that can learn (i.e., optimize model parameters) from data without explicit programming. AI/ML is neither one thing, such as a discrete computer application, nor one technology. Instead, it is a group of systems that is defined in part by the type of information used to train the model and the amount of involvement provided by human trainers. The suite of AI products in the spotlight since late 2022 are “generative AI” tools. Generative AI refers to AI systems that can generate content—such as written material, art, or computer code—from prompts using advanced techniques that help models better understand how data elements influence and depend on one another. While proponents of the technology believe it can transform delivery of services in industries that rely on both general and specialized knowledge alike, it has also raised concerns of misuse by bad actors and misrepresentation in the form of “deep fakes,” among others.AI Adoption and DiffusionThe effects of AI on the economy largely depend on the extent of AI use. The overall adoption of AI by businesses to this point has been limited. According to the Census Business Trends and Outlook Survey (BTOS), the number of businesses using AI rose from 3.7% in September 2023 to 5.4% in February 2024. Further, as shown in Figure 1, adoption across sectors has varied widely. The information sector’s usage of AI (18.1%) far surpasses usage in other sectors, but it nonetheless still indicates that a majority of information sector businesses are not yet using AI.Figure 1. Sector Usage of AI Selected Two-Week Period from February 2024 Survey/Source: Census Business Trends and Outlook Survey (BTOS).Notes: Sector names have been shortened for brevity. That use of AI has been somewhat limited does not necessarily indicate that it will remain so. In February 2024, businesses indicated an expected rate of AI use at 6.6% by fall 2024, an increase from earlier in the year. Other estimates indicate that certain types of AI—such as large language models, which underpin many generative AI tools—could be adopted by a large swath of businesses. One projection of future private U.S. investment in AI also indicates growth to $81.7 billion in 2025 from $47.4 billion in 2022. How much usage would be necessary to create structural shifts in the economy is uncertain, although the effects to this point appear somewhat limited.Further, just because a technology has the capacity for use does not necessarily indicate that it will be used quickly and broadly across economic activity or at all. The diffusion of past technologies has varied widely. According to the Federal Reserve Bank of St. Louis, the diffusion of AI appears to be following a pattern similar to personal computers or cloud computing, which were adopted slowly over multiple decades. The extent to which AI use becomes ubiquitous in the workplace is uncertain, but current evidence suggests that workplace use of roughly 40% or more could take over a decade. As such, the effects of AI on the economy may be relatively small for the time being but may grow over time. AI and the Labor MarketQuestions around how technologies may cause structural labor market shifts are long-standing. In the past, some economists voiced concerns about the extent to which automation could replace jobs, such as in manufacturing. Today, similar questions are being asked with regard to AI. As shown in Table 1, BTOS data indicate that while businesses that use AI report it replacing some number of tasks, employment effects in either direction are relatively small. This indicates that, as used currently, AI is not replacing workers on average. However, businesses report expectations that AI will have a growing impact on firm employment. Nonetheless, as has happened in past episodes of technological change, AI may result in a changing mix of available jobs, including the creation of new ones but not fewer total jobs in the long run.  Table 1. AI Effects in the WorkplaceResponses from February 2024 SurveySix Months Prior to SurveySix Months After SurveyResponses to “Number of Tasks Replaced by AI?” “Small Number”84.6%79.2%“Moderate Number”13.0%17.7%“Large Number”2.4%3.1%Responses to “Effects of AI on Firm Employment?”“Increase”2.8%6.5%“Decrease”2.6%6.1%“No Change”94.6%87.4%Source: BTOS.Notes: Questions asked only to firms that responded affirmatively to using AI. Potential Effects on ProductivityEconomists generally agree that the main avenue by which AI is likely to affect the economy is labor productivity. Many estimates suggest that AI can affect task productivity notably, although these effects may differ across skill levels. In general, studies tend to conclude that the use of AI increases task productivity and performance, although some studies have found that errors made by AI can be counterproductive. How total labor productivity may be affected by AI is harder to estimate, because it depends, in part, on how many and what types of tasks use AI. Goldman Sachs estimates that if 25% of total work tasks are automated by generative AI, labor productivity would increase 15%.AI also has the potential to impact total factor productivity (TFP), which accounts for the impact of technological growth. If the adoption of AI results in an increased pace of research and development, this would increase the growth rate of TFP and long-run economic growth. One study suggests that in the short run (10 years), the effects of AI on TFP would be a roughly 0.53% cumulative increase.AI and Economic GrowthProductivity growth is an important determinant of long-term economic growth and income. Gains in productivity allow for the more efficient production of goods and services, thereby increasing the productive capacity of the economy. Estimates of the effects of AI on U.S. gross domestic product (GDP) in the short and long run vary but are typically positive. The Goldman Sachs study estimates that this productivity increase could result in an up to 0.9% cumulative increase in GDP over the same time frame.When considering the longer run, more questions arise: How many and what types of new tasks will AI create in addition to automating existing ones? Will AI-based tasks become more advanced and therefore harder to learn? Will AI adoption be even across sectors or types of tasks? How will short-run effects of AI and policy decisions in the coming years affect future economic and policy responses? One study suggests that the effects of AI on GDP in the long run could vary substantially based on a number of factors, including which industries AI adoption is concentrated in and whether or not productivity gains from AI adoption are expected. The study suggests that GDP could rise to about 35% above baseline in the long run but could be much smaller depending on the scenario. AI and InequalityOwing to the expected effects on productivity growth, AI has the potential to increase total income in the economy. However, this does not mean that all economic actors will benefit. For example, in recent decades, productivity gains from technological innovation have largely been concentrated among high-income, skilled workers. It is not clear whether AI use would have a similar effect. On the one hand, evidence has suggested that within certain types of work, lower-skilled workers may benefit more from AI. For example, in a recent study the authors conducted an experiment in which customer support agents of varying experience and skill level used an AI-based conversational assistant. On average, this use of AI increased productivity significantly more for those with less experience or lower skill levels than for more experienced, higher-skilled workers. Thus, AI could potentially normalize outcomes across workers within businesses, in theory resulting in equalizing productivity and wages. On the other hand, AI is expected to generally produce higher productivity gains for higher income and skilled workers. For example, some research suggests that productivity gains from large language models could be significantly larger for higher-income workers. Policy ConsiderationsNumerous policy questions arise from the adoption and use of AI in the production process. Many of the most frequently asked questions involve labor market outcomes, as evidenced by recent House Oversight and Accountability Committee and Joint Economic Committee hearings on AI and the labor market. Questions often consider whether AI could structurally change the nature of work and, if so, how the costs and benefits of such a change might be distributed. Congress may wish to consider policies that would either prevent or mitigate changes to the labor market or otherwise provide assistance to those workers who are fully or partially displaced once any changes do take effect.",https://www.congress.gov/crs_external_products/IF/PDF/IF12762/IF12762.3.pdf
IN12537,2025-05-16T04:00:00Z,2025-06-21T07:06:19Z,Case Studies of How Generative Artificial Intelligence is Changing Work,"Generative Artificial Intelligence (GenAI) is a new technology that can generate new code, text, images, and other content based on machine learning systems and can be trained using large quantities of previous content.  During the 118th Congress, there were multiple congressional hearings on how GenAI technology may affect work and workers. GenAI became widely available only recently, and there are few case studies documenting how it is already changing the experience of work. This Insight summarizes two such case studies identified by CRS. In each of these studies, a new GenAI tool was intentionally rolled out to some workers before others so that researchers could compare workers who had access to the tool with similar workers who did not yet have access. In these studies, GenAI tools boosted job performance more for less experienced software developers and customer service agents. These findings highlight how much is still unknown about which workers will be most affected by GenAI technology.AI-Based Coding Assistance for Software DevelopersIn The Effects of Generative AI on High Skilled Work: Evidence from Three Field Experiments with Software Developers, Kevin Zheyuan Cui et al. describe the rollout of GitHub CoPilot to thousands of software developers at Microsoft in 2022, and to Accenture and an anonymous large electronics manufacturing company in 2023. GitHub CoPilot is a GenAI tool that makes suggestions of code and code documentation. Not every software developer given early access to the tool used it—between 60% and 70% of people who had early access tried it during the study period. Overall, those who had early access to GitHub CoPilot completed more coding tasks and completed them more successfully than their peers without early access. Among Microsoft software developers, the likelihood of trying the tool, the likelihood of continuing to use the tool after trying it, and the gains from tool access on productivity were all larger for more recent hires. AI-Based Conversation Assistance for Customer Support AgentsIn Generative AI at Work, Danielle Li, Erik Brynjolfsson, and Lindsey Raymond describe the rollout of an AI chat assistant (built on GPT-3 technology) to thousands of customer support agents in a large software company during 2020-2021. Customer support agents at this company—most based in the Philippines—helped U.S. clients deal with problems during calls lasting an average of 40 minutes. Customer support agents needed both technical knowledge and skill dealing with frustrated customers to do the job well. Their performance was measured by how quickly they successfully resolved customers’ issues, as well as by customer satisfaction surveys.The GenAI tool used at this company was trained on a dataset of previous customer-agent conversations from the best-performing agents. This tool listened to agents’ discussions with customers and offered real-time suggestions for what agents should say, which agents could either use or ignore. The agents who had access to the tool and were newer or less fluent in English used it to learn how do their job more like the best-performing customer service agents whose conversations were used to train the tool. The GenAI tool was most helpful for agents helping customers with “moderately rare” problems, in which new agents might have little experience but there was enough data to train the tool. Customer service agents with access to the GenAI tool who had two months of experience handled calls as well as agents without the tool who had six months of experience. Workers who used the tool to improve their performance continued to do better when the tool was unavailable. Agents based in the Philippines were able to interact with customers more like agents based in the United States. Using the GenAI tool appeared to make the customer service job less stressful for agents who used it: agents who had access to the tool saw a reduction in negative customer speech (such as yelling or swearing at agents) and in customer requests to speak with a manager. Agents who used the tool were also less likely to leave the company. A previous version of this Insight also summarized the paper Artificial Intelligence, Scientific Discovery, and Product Innovation. On May 16, 2025, the MIT Department of Economics released a statement saying, “the findings reported in this paper should not be relied on in academic or public discussions of these topics.”",https://www.congress.gov/crs_external_products/IN/PDF/IN12537/IN12537.2.pdf
R47843,2024-04-03T04:00:00Z,2025-06-21T03:25:57Z,Highlights of the 2023 Executive Order on Artificial Intelligence for Congress,"On October 30, 2023, the Biden Administration released Executive Order (E.O.) 14110 on Safe, Secure, and Trustworthy Development and Use of Artificial Intelligence. It establishes a government-wide effort to guide responsible artificial intelligence (AI) development and deployment through federal agency leadership, regulation of industry, and engagement with international partners. The E.O. directs over 50 federal entities to engage in more than 100 specific actions to implement the guidance set forth across eight overarching policy areas.Safety and security. The E.O. promotes the development and implementation of repeatable processes and mechanisms to understand and mitigate risks related to AI adoption, including with respect to biosecurity, cybersecurity, national security, and critical infrastructure. Innovation and competition. The E.O. compels actions to attract AI talent to the United States, understand novel intellectual property (IP) questions, protect inventors and creators, and promote AI innovation, including at startups and small businesses. Worker support. The E.O. states that AI adoption may be disruptive to the workforce and directs agencies to research and develop potential mitigations against such disruptions. Consideration of AI bias and civil rights. The E.O. states that AI models may perpetuate biases and their implementation may lead to civil rights violations. The E.O. includes a section on equity and civil rights considerations for use of AI in the criminal justice system and the administration of federal government programs and benefits. Consumer protection. The E.O. instructs agencies to enforce existing, technology-agnostic authorities in an effort to minimize harms to consumers, and to identify needed authorities related to AI. Privacy. The E.O. calls for the evaluation and mitigation of privacy risks—potentially exacerbated by AI—associated with the collection, use, and retention of user data. Federal use of AI. The E.O. requires the Office of Management and Budget (OMB) to establish an interagency council to coordinate AI use by federal agencies and develop guidance on AI governance and risk management activities for agencies. It acknowledges the ubiquity of generative AI (GenAI) tools, and directs agencies to move toward adoption with safeguards in place. The E.O. also calls for additional agency hiring and training activities to increase the AI workforce capacity across the federal government. International leadership. The E.O. declares that the United States should be a global leader in AI development and adoption by engaging with international allies and partners, leading efforts to develop common AI regulatory and accountability principles, and advancing responsible global technical standards for AI.",none found
IF12899,2025-02-05T05:00:00Z,2025-06-21T05:08:24Z,Data Centers and Cloud Computing: Information Technology Infrastructure for Artificial Intelligence,"The advancement of artificial intelligence (AI), a “critical and emerging technology,” presents policy considerations for U.S. leadership, with implications for economic competitiveness and national security. AI systems, including their development, model training, deployment, operation, applications, and services, rely on an information technology (IT) infrastructure with components of hardware, software, networks, data, and facilities. Data centers are the primary means to house much of this IT backbone. Internet-based remote computing services (i.e., cloud computing) enable AI developers and users to access computing resources hosted in geographically distributed data centers. Not only may AI innovation and competition hinge on the availability of and access to advanced, secure, and sustainable computing resources, but such IT infrastructure may also be deemed “a strategic national asset.” Related issues have attracted congressional attention in recent years (e.g., a Senate committee hearing on advanced computing research and a House committee hearing on “powering AI”).The U.S. government has increasingly focused on policy directions to support building a robust, domestic AI infrastructure. For example, President Biden issued Executive Order (E.O.) 14141, “Advancing United States Leadership in Artificial Intelligence Infrastructure,” on January 14, 2025, providing a federal plan to build AI infrastructure in the United States. On January 21, 2025, President Trump announced a private joint venture with potential investment of up to $500 billion to fund AI infrastructure, including plans to build up to 20 data centers in the country. The National Telecommunications and Information Administration (NTIA), a Department of Commerce agency serving as the President’s principal advisor on telecommunication and information policies, issued a request for comments (RFC) to inform policymaking for “sustainable, resilient and secure” growth of data centers to power critical and emerging technologies, including AI. The RFC received 58 comments, ranging from the importance of allocating an estimated $175 billion of potential funds to U.S.-backed global AI infrastructure projects to challenges faced by domestic data centers, such as regulatory obstacles for siting new facilities and access to energy, supply chains, and operational workforce.This In Focus introduces the use of data center and cloud computing infrastructure for AI development and AI-enabled services and selected policy issues for congressional consideration. For more on AI technologies and policies, see CRS Report R47644, Artificial Intelligence: Overview, Recent Advances, and Considerations for the 118th Congress; for more on energy issues related to selected data centers, see CRS Report R45863, Bitcoin, Blockchain, and the Energy Sector.Overview of Data CentersIn its simplest form, a data center is a facility that houses and powers a large computer system. Data centers have evolved to house multiple enterprise-level, interconnected computer servers (e.g., a cluster of servers called a server farm). Personal computers and smart devices connect users to these servers to access online services (e.g., websites, emails, and file sharing). Many data centers have expanded to support cloud computing services, allowing users to remotely access computing resources such as data processing chips, software, data storage, networks, and applications and services hosted by these centers.The term data center has been defined in federal laws in the context of energy efficiency and federal use of data centers. For instance, the Energy Independence and Security Act of 2007 (P.L. 110-140, §453(a)(1)) defines a data center as a facility that “contains electronic equipment used to process, store, and transmit digital information.” In its guidance (M-25-03) for federal agencies to implement the Federal Data Center Enhancement Act of 2023 (P.L. 118-31, §5302), the Office of Management and Budget specified that a data center (1) is composed of permanent structures and operates in a fixed location; (2) houses IT equipment, including servers and other high-performance computing devices, or data storage devices; and (3) hosts information and information systems accessed by other systems or by users on other devices.Data Centers for AIThe ever-increasing demand for data storage and processing capacities, especially for intensive computational tasks such as AI training, has led to construction and operation of hyperscale data centers. According to industry analysts, to be considered a hyperscale data center, a facility must contain at least 5,000 servers and occupy at least 10,000 square feet of physical space, with a power demand exceeding 100 megawatts (MW). If a data center had a continuous power demand of 100 MW for 24 hours, it would consume 2,400 MW-hours (MWh) of energy.These large facilities provide powerful computing resources (such as memories, central processing units [CPUs], and graphics processing units [GPUs]) to handle vast amounts of data and large-scale workloads. (CPU and GPU are two major types of computing chips found in most personal computers and servers.) General-purpose workloads typically require only a CPU, while a GPU is generally considered better than a CPU to handle AI computational tasks. According to the chip manufacturer Nvidia, an AI-ready data-center server can support one to eight high-performance GPUs, each of which consumes hundreds of watts of electricity. Cutting-edge chip technologies also support high-speed (at the gigabit-per-second level) GPU-to-GPU data communication among hundreds of GPUs across multiple servers, enabling the creation of a massive AI server farm in a data center.In E.O. 14141, the term AI data center means a data center used primarily to develop or operate AI, and the term frontier AI data center means an AI data center capable of being used to develop an AI model that matches or surpasses the state-of-the-art AI model with regard to its performance or computational resources used in its development. These frontier AI models and related services typically require more power than traditional IT operations supported by data centers (e.g., data storage, retrieval, and transmission). For example, ChatGPT was estimated to use 2.9 watt-hours to respond to a user query, while a traditional Google search query uses about 0.3 watt-hours.Multiple industry reports indicate that data processing demands of AI and related cloud computing services have spurred new construction and upgrades of data centers. The computing resources hosted by these centers would in turn lead to increased power demand. For example, one report estimated that the computing capacity (measured by the power demand) of data centers under construction in North America in the first half of 2024 reached a record-high 3,872 MW, up by 69% from a year earlier. Nearly 80% of this capacity has been pre-leased, with AI and cloud service providers contributing to significant portions of such demand. A report commissioned by the Department of Energy estimated that data centers accounted for about 4.4% of total U.S. electricity consumption (or about 176 million MWh) in 2023, “equivalent to the average annual consumption of 14 million households.” According to another report, new hyperscale data centers have been built with capacities from 100 to 1,000 MW, “roughly equivalent to the load from 80,000 to 800,000 homes.”Investments in AI InfrastructureMany experts believe that AI development and operation require significant capital investment. According to the annual AI Index Report 2024, costs of training large foundation models alone could “run into millions of dollars and are rising.” Given rental prices charged by major cloud service providers for accessing computing hardware, the report estimated that training costs in 2023 for OpenAI’s GPT-4 and Google’s Gemini Ultra were around $78 million and $191 million, respectively (excluding other costs, such as data acquisition and labor). In a January 24, 2025, Facebook post, Meta’s CEO revealed that the company was building a 2,000-MW data center and planned to spend $60-$65 billion in AI capital expenditures and acquire over 1.3 million GPUs by the end of 2025. Three days earlier at the White House, a group of companies announced a joint venture called Stargate, planning to invest up to $500 billion over the next four years to build AI infrastructure in the United States. Stargate’s first data center, reportedly under construction, would be used by OpenAI and operated by Oracle (one of the largest cloud-based database vendors).The tech industry and financial market have raised questions about large investments in AI infrastructure, due to the emergence of high-performing AI models developed by the less-known Chinese company DeepSeek. The company claimed in a non-peer-reviewed technical report that it has developed a large language model (LLM) called DeepSeek-V3 using efficient and cost-effective approaches. The model required 2.8 million GPU hours (equal to about 57 days) for its full training on a cluster of 2,048 Nvidia H800 GPUs. Assuming the cloud rental price at $2 per GPU hour, DeepSeek reported its total training cost for the model was $5.6 million—a fraction of what is currently spent by leading U.S. AI companies (e.g., $78 million for GPT-4). Nvidia reportedly developed the H800 chip in March 2023 as a modified version of its more powerful H100 GPU to comply with U.S. export control regulations. It is no longer permissible to export the H800 chip since the Department of Commerce’s Bureau of Industry and Security updated its rules on advanced computing chips in October 2023 (see Category 3A090 in Supplement No. 1 to 15 C.F.R. Part 774).According to its report, DeepSeek evaluated its V3 model against several leading LLMs, including those developed by Alibaba, Meta, OpenAI, and Anthropic. In six benchmark tests in language understanding, graduate-level science and mathematics questions, high school-level math competition questions, coding, and memory-chip security, the company claimed that DeepSeek-V3 outperformed the competitors in three and ranked second in the other three. The widely reported relatively low-cost advancement in AI model training has raised questions about the necessity of large AI investments, the efficiency of AI development, and U.S. leadership in the field.Selected Policy Issues for CongressAI infrastructure, including data centers, cloud computing services, and energy resources, is vital for AI development and operation. The criticality of such infrastructure raises policy issues such as data security, infrastructure access and security, energy reliability and efficiency, and costs of building such infrastructure, all of which implicate national security interests. Investment, construction, and operation of AI infrastructure may impact job and workforce development opportunities, and the economies and natural resources of local communities near data centers.Previous Congresses saw bills to assess, understand, and address potential impacts of growth in AI, data centers, and needs for advanced computing resources. For example, the Department of Energy AI Act (S. 4664, 118th Congress) would have required the Secretary of Energy to report to Congress on data centers for advanced computing, their hardware and software needs for AI, and national security risks. The Remote Access Security Act (H.R. 8152, 118th Congress) would have expanded export controls to include remote access (e.g., through a cloud computing service) to a commodity, software, or technology for purposes such as training AI models.",https://www.congress.gov/crs_external_products/IF/PDF/IF12899/IF12899.1.pdf
R47849,2023-11-22T05:00:00Z,2025-06-21T04:37:01Z,"Artificial Intelligence in the Biological Sciences: Uses, Safety, Security, and Oversight","Artificial intelligence (AI) is a term generally thought of as computerized systems that work and react in ways commonly thought to require intelligence. AI technologies, methodologies, and applications can be used throughout the biological sciences and biology research and development (R&D), including in engineering biology (e.g., the application of engineering principles and the use of systematic design tools to reprogram cellular systems for a specific functional output). This has enabled R&D advances across multiple application areas and industries. For example, AI can be used to analyze genomic data (e.g., DNA sequences) to determine the genetic basis of a particular trait and potentially uncover genetic markers linked with those traits. It has also been used in combination with biological design tools to aid in characterizing proteins (e.g., 3-D structure) and for designing new chemical structures that can enable specific medical applications, including for drug discovery. AI can also be used across the scientific R&D process, including the design of laboratory experiments, protocols to run certain laboratory equipment, and other “de-skilling” aspects of scientific research. The convergence of AI and other technologies associated with biology can lower technical and knowledge barriers and increase the number of actors with certain capabilities. These capabilities have potential for beneficial uses while at the same time raising certain biosafety and biosecurity concerns. For example, some have argued that using AI for biological design can be repurposed or misused to potentially produce biological and chemical compounds of concern. Both AI and engineering biology are multidisciplinary fields that build on advances in multiple scientific disciplines and technical developments that each have associated benefits and risks. As they converge, those benefits and risks may compound and create unique uncertainties, challenge governance systems, and, in some instances, raise new biosafety and biosecurity concerns. The U.S. Intelligence Community’s 2023 Annual Threat Assessment stated that the fields of AI and biotechnology are “being developed and are proliferating faster than companies and governments can shape norms, protect privacy, and prevent dangerous outcomes.” AI’s use in biology sits within a broader debate on how best to manage biosafety and biosecurity, including whether that use may lower technical and knowledge barriers and increase the likelihood of malign use. The potential for the use of AI in conjunction with biological design and other types of scientific research and experimentation has prompted recommendations from various groups on how to prevent the misuse of AI applications in biology and science more broadly. These include keeping a human “in the loop,” controlling access to DNA sequences and synthesis capabilities, and governance mechanisms that restrict, or monitor, who can access certain AI applications and biological design tools. Numerous bills introduced in the 118th Congress address the risks, benefits, and strategic competitiveness of AI more generally, while H.R. 4704, S. 2399, and S. 2346 focus more specifically on AI and biological threats. Other policy considerations for Congress could include the following: Whether AI, and its use in biology and other scientific fields, should be regulated broadly across all use cases and areas of development, regulated on a case-by-case basis focused on particular application areas or end-use products, or whether current biosafety and biosecurity oversight frameworks are sufficient. Whether a broad risk management approach examining the R&D system as a whole is most appropriate, including at what stage oversight is warranted (e.g., basic research stage, prototype stage, or prior to release of a final product) and what entity might conduct such oversight (e.g., a federal agency or a system of self-governance incentives). Whether to provide an agency (or agencies) authority to conduct oversight of various aspects of AI, create a new agency with authority over AI, or authorize agencies under current law to enforce certain oversight responsibilities. Authorizing agencies under current law may require additional coordination among federal agencies in order to establish oversight responsibilities. Other types of self-governance incentives could also be examined.Oversight and governance of the use of AI in the biological sciences involves potential benefits and risks associated with the biological design capabilities of AI models, as well as the feasibility of producing AI-generated designs that pose possible biosecurity risks. Regulating AI broadly, or the use of AI in one area of R&D, could impact other areas of R&D in specific or unintended ways. For example, limiting access to AI models, restricting the types of data an AI model may be trained on, or limiting the capabilities an AI model is allowed to execute could each impact a biological design tool’s capability.",https://www.congress.gov/crs_external_products/R/PDF/R47849/R47849.2.pdf
R47644,2023-08-04T04:00:00Z,2025-06-21T02:11:02Z,"Artificial Intelligence: Overview, Recent Advances, and Considerations for the 118th Congress","Artificial intelligence (AI)—a term generally thought of as computerized systems that work and react in ways commonly thought to require intelligence—can encompass a range of technologies, methodologies, and application areas, such as natural language processing, facial recognition, and robotics. The concept of AI has existed for decades, with the term first being coined in the 1950s, followed by alternating periods of much development and lulls in activity and progress. A notable area of recent advancement has been in generative AI (GenAI), which refers to machine learning (ML) models developed through training on large volumes of data in order to generate content. Technological advancements in the underlying models since 2017, combined with the open availability of these tools to the public in late 2022, have led to widespread use. The underlying models for GenAI tools have been described as “general-purpose AI,” meaning they can be adapted to a wide range of downstream tasks. Such advancements, and the wide variety of applications for AI technologies, have renewed debates over appropriate uses and guardrails, including in the areas of health care, education, and national security.AI technologies, including GenAI tools, have many potential benefits, such as accelerating and providing insights into data processing, augmenting human decisionmaking, and optimizing performance for complex systems and tasks. GenAI tools, for example, are increasingly capable of performing a broad range of tasks, such as text analysis, image generation, and speech recognition. However, AI systems may perpetuate or amplify biases in the datasets on which they are trained; may not yet be able to fully explain their decisionmaking; and often depend on such vast amounts of data and other resources that they are not widely accessible for research, development, and commercialization beyond a handful of technology companies.Numerous federal laws on AI have been enacted over the past few Congresses, either as standalone legislation or as AI-focused provisions in broader acts. These include the expansive National Artificial Intelligence Initiative Act of 2020 (Division E of P.L. 116-283), which included the establishment of an American AI Initiative and direction for AI research, development, and evaluation activities at federal science agencies. Additional acts have directed certain agencies to undertake activities to guide AI programs and policies across the federal government (e.g., the AI in Government Act of 2020, P.L. 116-260; and the Advancing American AI Act, Subtitle B of P.L. 117-263). In the 117th Congress, at least 75 bills were introduced that either focused on AI and ML or had AI/ML-focused provisions. Six of those were enacted. In the 118th Congress, as of June 2023, at least 40 bills had been introduced that either focused on AI/ML or contained AI/ML-focused provisions, and none has been enacted. Collectively, bills in the 118th Congress address a range of topics, including federal government oversight of AI; training for federal employees; disclosure of AI use; export controls; use-specific prohibitions; and support for the use of AI in particular sectors, such as cybersecurity, weather modeling, wildfire detection, precision agriculture, and airport safety.A primary consideration under debate in the United States and internationally is whether and how to regulate AI technologies. The European Union’s draft Artificial Intelligence Act would broadly take a risk-based approach to regulatory requirements and prohibitions for certain uses. In the United States, previously introduced legislation has sought to require impact assessments and reporting for automated decision systems—including but not limited to AI systems—in critical areas (e.g., health care, employment, and criminal justice). Other perspectives on AI regulation have suggested a sector-specific approach with interagency coordination. A general concern for Congress might be how to approach AI regulatory efforts in a way that balances support for innovation and beneficial uses while minimizing current and future harms.Additional considerations for the 118th Congress might include whether the current federal government mechanisms are sufficient for AI oversight and policymaking, the role of the federal government in supporting AI research and development, the potential impact of AI technologies on the workforce, disclosure of AI use, testing and validation of AI systems, and potential ways to support the development of trustworthy and responsible AI.",https://www.congress.gov/crs_external_products/R/PDF/R47644/R47644.1.pdf
IF12468,2023-08-17T04:00:00Z,2025-06-21T02:14:03Z,Artificial Intelligence (AI) in Federal Election Campaigns: Legal Background and Constitutional Considerations for Legislation,"IntroductionFederal campaign finance law does not specifically regulate the use of artificial intelligence (AI) in political campaign advertising. As technology continues to evolve, concerns have grown regarding the use of AI-generated campaign ads and their potential to spread misinformation. At the same time, there are questions about whether regulation of such ads would run afoul of the First Amendment. This CRS In Focus discusses provisions of federal campaign finance law that may be relevant should Congress consider regulating AI-generated campaign ads. It then discusses pivotal Supreme Court rulings on campaign finance law and constitutional considerations for possible legislation. For a related policy discussion, see CRS product, Artificial Intelligence (AI) and Campaign Finance Policy: Recent Developments, by R. Sam Garrett.Federal Campaign Finance LawThe Federal Election Campaign Act (FECA or Act), codified at 52 U.S.C. §§ 30101–30146, does not specifically regulate the use of AI in political campaign ads. Two FECA provisions, however, may be relevant to this issue: the prohibition on fraudulent misrepresentation of campaign authority and the requirement of disclaimers, which are statements of attribution that appear directly on certain campaign communications.FECA Prohibition on Fraudulent Misrepresentation of Campaign AuthorityFECA prohibits a federal office candidate, including employees and agents of such a candidate, from fraudulently misrepresenting another candidate or political party “on a matter which is damaging to such other candidate or political party.” The Act further prohibits anyone from fraudulently soliciting campaign contributions whereby the solicitor misrepresents that he or she is fundraising on behalf of a candidate or party. 52 U.S.C. § 30124. On August 16, 2023 the Federal Election Commission (FEC) published a petition for rulemaking to amend its regulation on fraudulent misrepresentation of campaign authority, 11 C.F.R. §110.16, to clarify that the related statute, 52 U.S.C. § 30124, applies if “candidates or their agents fraudulently misrepresent other candidates or political parties through deliberately false AI-generated content in campaign ads or other communications.” It approved the petition on August 10, 2023. (On June 22, the FEC had discussed, but not approve, a similar petition.)FECA Disclaimer RequirementsFECA requires that any public political advertising financed by a political committee—including candidate committees—include disclaimers. FECA and Supreme Court precedent define political committee to include “any committee ... or other group of persons that receives contributions or makes expenditures aggregating in excess of $1,000 during a calendar year” whose major purpose is to elect federal candidates to office. 52 U.S.C. § 30101(4); see Buckley v. Valeo, 424 U.S. 1, 79 (1976). FECA further defines contribution and expenditure as monies or anything of value “for the purpose of influencing any election for Federal office.” 52 U.S.C. § 30101(8), (9).For radio and television advertisements by candidate committees, FECA generally requires that the communication state who financed the ad, along with an audio statement by the candidate identifying the candidate and stating that the candidate “has approved” the message. In the case of television ads, the candidate statement is also required to be conveyed by an unobscured, full-screen view of the candidate making the statement or, if the candidate message is conveyed by voice-over, accompanied by a clearly identifiable image of the candidate, along with a written message of attribution at the end of the communication. 52 U.S.C. § 30120(a).In addition, regardless of the financing source, FECA requires a disclaimer on (1) communications that expressly advocate for the election or defeat of a clearly identified candidate, (2) electioneering communications (defined to include broadcast ads that refer to a clearly identified federal candidate that are run 60 days before a general election or 30 days before a primary), and (3) public communications that solicit contributions. These communications can include ads financed by outside groups, corporations, or labor unions. For such ads, FECA generally requires that a disclaimer clearly state certain contact information of the entity that paid for the communication and that the communication was not authorized by any candidate or candidate committee. In radio and television advertisements, such disclaimers are required to include, in a clearly spoken manner, an audio statement saying who is responsible for the content of the advertising. In television ads, the statement is required to be conveyed by an unobscured, full-screen view of a representative of the entity paying for the ad, in a voice-over, along with a written message of attribution at the end of the communication. 52 U.S.C. § 30120(a), (c), (d).Effective March 1, 2023, the FEC promulgated new regulations that broaden the disclaimer requirements for public internet communications. Previously, the regulations generally required disclaimers on public communications—defined to include ads that are “placed for a fee on another person’s website”—that were made by political committees, contained express advocacy, or solicited campaign contributions. The new regulations specify that this requirement also applies to “communications placed for a fee on another person’s ... digital device, application, or advertising platform.” 87 Fed. Reg. 77467–77480 (Dec. 19, 2022).Regardless of whether a campaign communication is created with AI, FECA’s disclaimer requirements would apply as discussed. However, the Act does not require such disclaimers to indicate that the ad was created with AI. FECA PenaltiesIn addition to a series of civil penalties, FECA sets forth criminal penalties for knowing and willful violations of the Act. Generally, FECA provides that any person who knowingly and willfully violates any provision of the Act that involves the making, receiving, or reporting of any contribution, donation, or expenditure of $25,000 or more per calendar year shall be fined under Title 18 of the U.S. Code, imprisoned for not more than five years, or both. If the amount involved is $2,000 or more per calendar year, but less than $25,000, the Act provides for a fine or imprisonment for not more than one year, or both. Should Congress amend FECA to regulate AI-generated campaign ads, unless otherwise provided in the legislation, FECA’s civil and criminal penalties would apply.Constitutional Considerations for LegislationIn the 118th Congress, legislation has been introduced that would regulate AI in federal election campaigns. For example, H.R. 3044 and S. 1596, which are companion bills, would amend FECA’s disclaimer requirements to require additional disclaimers. Specifically, for an ad that contains an image or video generated, entirely or in part, by AI, the legislation would require the ad to include a statement indicating that fact.Should Congress consider legislation to amend FECA establishing an AI disclaimer requirement, the Supreme Court’s campaign finance jurisprudence may be relevant in evaluating the constitutional bounds of such legislation. For example, the Court upheld the facial validity of FECA’s disclaimer requirements against a First Amendment challenge, determining that the disclaimer requirements “bear[] a sufficient relationship to the important governmental interest of shedding the light of publicity on campaign financing.’” McConnell v. FEC, 540 U.S. 93, 231 (2003). Similarly, the Court upheld FECA’s disclaimer requirements as applied to a film regarding a presidential candidate and related promotional broadcast ads. Quoting Buckley and McConnell, the Court in Citizens United determined that while disclaimer requirements may burden the ability to speak under the First Amendment, they “impose no ceiling on campaign-related activities” and “do not prevent anyone from speaking.” According to the Court, FECA’s disclaimer requirements “provid[e] the electorate with information” and “insure that the voters are fully informed” about who is speaking. Moreover, they facilitate the ability of a listener or viewer to judge more effectively the arguments they are hearing, the Court observed. Citizens United v. FEC, 558 U.S. 310, 368 (2010). In McConnell and Citizens United, the Court applied a standard of “exacting scrutiny” that requires a substantial relation between the disclaimer requirement and a sufficiently important governmental interest. These precedents suggest that courts could uphold the constitutionality of a FECA AI-disclaimer requirement to the extent the government could show that the requirement furthers the informational interests of the electorate. However, it is uncertain whether courts will determine that notifying the electorate that an ad was created with AI is as sufficiently an important governmental interest as informing the electorate as to who financed or approved of an ad, as mandated by the current FECA disclaimer requirements. Exacting scrutiny also requires a court to evaluate the burden on speech. As the Court appeared to rely on the fact that FECA’s current disclaimer requirements did not prevent anyone from speaking, if such a requirement is so burdensome that it impedes the ability of a candidate or group to speak—for example, if a required disclaimer comprises a relatively long period of time in an ad—it could violate the First Amendment. Citizens United v. FEC, 558 U.S. at 366–71.Possibly casting further doubt on the constitutionality of an AI disclaimer requirement, the Court recently invalidated a state disclosure law under a potentially more rigorous standard of exacting scrutiny that requires a “narrow tailoring” to a sufficiently important governmental interest. Americans for Prosperity Foundation v. Bonta, 141 S. Ct. 2373, 2389 (2021). While Bonta is not a campaign finance case, some lower courts have since applied this version of exacting scrutiny in cases challenging campaign disclaimer laws. In evaluating an AI disclaimer requirement under this potentially more rigorous standard, a court might be less likely to uphold the law. Nonetheless, some appellate courts have approved of campaign finance disclaimer laws under this narrow tailoring standard. See No on E v. Chiu, 62 F.4th 529, 533 (9th Cir. 2023) and Gaspee Project v. Mederos, 13 F.4th 79, 95–96 (1st Cir. 2021), cert. denied 142 S. Ct. 2647 (2022). In contrast to a disclaimer requirement, it appears that courts would likely determine that a prohibition on AI-generated campaign ads is unconstitutional under the First Amendment. In evaluating a prohibition on certain campaign communications, the Supreme Court applied a “strict scrutiny” standard of review. Strict scrutiny requires the government to show that the law is the least restrictive means to achieve a compelling interest, which is a difficult standard to meet. Hence, applying strict scrutiny, the Court invalidated a FECA provision that prohibited corporations and unions from directly funding independent expenditures and electioneering communications. Citizens United v. FEC, 558 U.S. at 372. Accordingly, it appears that a prohibition on AI-generated campaign ads would likely be invalidated under a strict scrutiny standard of review unless the government could show that the law achieves a compelling government interest.",https://www.congress.gov/crs_external_products/IF/PDF/IF12468/IF12468.1.pdf
R47569,2023-05-23T04:00:00Z,2025-06-21T04:36:20Z,Generative Artificial Intelligence and Data Privacy: A Primer,"generative AI, generative artificial intelligence, data privacy, data protection, data protections, artificial intelligence, chatbot, ChatGPT, Bard, Stable Diffusion, LLaMA, DALL-E, GPT-3, GPT-4, large language models, LLMs, LLM, AI, Bing AI, Midjourney, text-to-speech, text-to-video, algorithms, machine learning, artificial intelligence, deepfake, California Consumer Privacy Act (CCPA), privacy laws, privacy law, European Union AI Act, Artificial Intelligence Act, EU AI Act, models, General Data Protection Regulation (GDPR), AI model, AI regulation, regulation of AI, chatbots, deepfakes, generative AI, generative artificial intelligence, data privacy, data protection, data protections, artificial intelligence, ChatGPT, Bard, Stable Diffusion, LLaMA, DALL-E, GPT-3, GPT-4, large language models, LLMs, LLM, AI, Bing AI, Midjourney, text-to-speech, text-to-video, algorithms, machine learning, artificial intelligence, California Consumer Privacy Act (CCPA), privacy laws, privacy law, European Union AI Act, Artificial Intelligence Act, EU AI Act, models, General Data Protection Regulation (GDPR), AI model,generative AI, generative artificial intelligence, data privacy, data protection, data protections, artificial intelligence, ChatGPT, Bard, Stable Diffusion, LLaMA, DALL-E, GPT-3, GPT-4, large language models, LLMs, LLM, AI, Bing AI, Midjourney, text-to-speech, text-to-video, algorithms, machine learning, artificial intelligence, California Consumer Privacy Act (CCPA), privacy laws, privacy law, European Union AI Act, Artificial Intelligence Act, EU AI Act, models, General Data Protection Regulation (GDPR), AI model,generative AI, generative artificial intelligence, data privacy, data protection, data protections, artificial intelligence, ChatGPT, Bard, Stable Diffusion, LLaMA, DALL-E, GPT-3, GPT-4, large language models, LLMs, LLM, AI, Bing AI, Midjourney, text-to-speech, text-to-video, algorithms, machine learning, artificial intelligence, California Consumer Privacy Act (CCPA), privacy laws, privacy law, European Union AI Act, Artificial Intelligence Act, EU AI Act, models, General Data Protection Regulation (GDPR), AI model,generative AI, generative artificial intelligence, data privacy, data protection, data protections, artificial intelligence, ChatGPT, Bard, Stable Diffusion, LLaMA, DALL-E, GPT-3, GPT-4, large language models, LLMs, LLM, AI, Bing AI, Midjourney, text-to-speech, text-to-video, algorithms, machine learning, artificial intelligence, California Consumer Privacy Act (CCPA), privacy laws, privacy law, European Union AI Act, Artificial Intelligence Act, EU AI Act, models, General Data Protection Regulation (GDPR), AI model,generative AI, generative artificial intelligence, data privacy, data protection, data protections, artificial intelligence, ChatGPT, Bard, Stable Diffusion, LLaMA, DALL-E, GPT-3, GPT-4, large language models, LLMs, LLM, AI, Bing AI, Midjourney, text-to-speech, text-to-video, algorithms, machine learning, artificial intelligence, California Consumer Privacy Act (CCPA), privacy laws, privacy law, European Union AI Act, Artificial Intelligence Act, EU AI Act, models, General Data Protection Regulation (GDPR), AI model,generative AI, generative artificial intelligence, data privacy, data protection, data protections, artificial intelligence, ChatGPT, Bard, Stable Diffusion, LLaMA, DALL-E, GPT-3, GPT-4, large language models, LLMs, LLM, AI, Bing AI, Midjourney, text-to-speech, text-to-video, algorithms, machine learning, artificial intelligence, California Consumer Privacy Act (CCPA), privacy laws, privacy law, European Union AI Act, Artificial Intelligence Act, EU AI Act, models, General Data Protection Regulation (GDPR), AI model,",https://www.congress.gov/crs_external_products/R/PDF/R47569/R47569.4.pdf
IF12399,2023-05-10T04:00:00Z,2025-06-21T01:25:46Z,"Automation, Artificial Intelligence, and Machine Learning in Consumer Lending","Financial firms may use algorithms—pre-coded sets of instructions and calculations that are executed automatically—to enhance consumer loan underwriting, the process of evaluating the likelihood that applicants will make timely loan repayments. Lenders may rely upon forms of automated analysis to help decide whether to offer consumers loans and at what terms. Faster computing power, internet-based products, and cheaper data storage at scale have increased the prevalence of algorithms.This In Focus discusses developments in automated decisionmaking, artificial intelligence (AI), and machine learning (ML) in consumer loan underwriting. First, it focuses on market developments, then it discusses the current regulatory framework, then finally, it highlights selected policy issues.Market DevelopmentsSince the 1970s, consumer loan underwriting has become more automated, first with the increasing use of credit scores and more recently with new data and technologies. Credit scores are a (numeric) metric calculated with information in consumer credit reports and prepared for lenders to determine the likelihood of loan default. New technological innovations have been used to update automated processes, in some cases beyond traditional numeric credit scores. For example, for some lenders, the internet has been incorporated to accept applications, and new data sources are used to conduct consumer loan underwriting. Alternative data generally refers to information that may be used to determine a consumer’s creditworthiness that the national credit reporting agencies—Equifax, Experian, and TransUnion—have not traditionally used when calculating credit scores for consumers. Further, AI and ML technologies have advanced rapidly in recent decades. AI technologies are computerized systems that work and react in ways commonly thought to require intelligence, such as solving complex problems in real-world situations. ML is often referred to as a subfield of AI with algorithms designed to automatically improve their performance through experience with little or no human input.These technological developments potentially allow for greater speed, accuracy, and confidence in loan decisions. They are currently used more frequently in fintech products than in more traditional consumer lending products, particularly ML models and alternative data. Fintech (short for financial technology) refers to advances in technology incorporated into financial products and services. Many companies—both traditional financial firms and new technology-focused entrants to the market—are developing fintech products, making it a subject of increased interest for the public and policymakers. ML Models in Consumer Loan UnderwritingConsumer loan underwriting can potentially be enhanced by ML models. ML models could improve efficiency and performance and reduce costs for financial institutions, potentially expanding credit access or making credit less expensive for some consumers. ML models could make consumer underwriting decisions more accurate by identifying new patterns, such as changing credit conditions, and by automatically updating the models to make more accurate underwriting assessments. However, ML models can also introduce risks. One risk is a lack of explainability, the inability to explain why programs make particular decisions. Another risk is dynamic updating, which is when models evolve over time without oversight. ML models also raise concerns that they may not perform as intended, possibly resulting in higher loan losses in new market environments or discrimination against protected groups. Current Federal Regulatory FrameworkThe Consumer Financial Protection Bureau (CFPB) is the primary consumer protection regulator for consumer financial products and services. One of the CFPB’s statutory objectives is to ensure that “markets for consumer financial products and services operate transparently and efficiently to facilitate access and innovation.” The CFPB has the authority in consumer financial markets to write regulations and enforce the law for both bank and nonbank financial institutions. However, the CFPB’s supervisory authority to examine financial institutions for consumer protection compliance varies based on the charters, activities, and size of institutions. Therefore, financial regulators may monitor some nonbank fintech companies less than traditional banks.Regulatory UncertaintyMany financial laws and regulations that existed prior to recent ML technological developments have led to questions concerning their effectiveness achieving their designed policy goals as these potentially beneficial technologies evolve. Relevant laws and regulations may need to be reconsidered or updated in response to the future use of ML models in consumer loan underwriting. This often involves balancing efforts to encourage innovation while protecting consumers.Federal financial regulators have been monitoring ML models in consumer lending. In March 2021, the bank and credit union regulators, along with the CFPB, requested information on financial institutions’ use of AI, including ML models. In April 2023, the CFPB, along with other federal agencies, published a joint statement emphasizing that existing legal authorities still apply to automated systems and AI.Consequently, some financial institutions, particularly many banks or other highly regulated parts of the financial system, may choose not to use ML models to approve or reject applicants for consumer credit, even if they are more accurate or efficient, due to regulatory uncertainty and compliance risks.Policy IssuesThe use of ML algorithms in credit underwriting has raised a number of policy issues of interest for financial regulators and Congress. ML Models and Explainability ConcernsThe ability of regulators or other outside parties to understand what an ML program did, and why, may be limited or nonexistent. This poses a significant challenge for companies using ML programs to ensure that they will produce outcomes that comply with applicable laws and regulations. When a lender denies a loan application, the lender must send an adverse action notice to the applicant explaining the reason for the denial. Some question how well lenders will understand and be able to explain the reasons for adverse actions resulting from ML algorithms. To address this issue, some observers assert that regulators should set standards for how ML programs are developed, tested, and monitored, although debate exists about what these standards should include. Concerns exist about ML model fairness; the ability to provide more algorithm transparency; and developing processes to assess ML models, for example, for fairness, reliability, privacy, and security. In May 2022, the CFPB issued guidance clarifying that lenders using “complex algorithms” still need to comply with adverse action notice requirements. Algorithmic Bias and Fair LendingConsumer loan underwriting models using ML can introduce fair lending risks due to biases in data or model development. ML models may have training data biases, which is when a model has biases due to the limited or flawed dataset it was developed on. Historical data can reflect historical biases, potentially creating models that discriminate against protected classes. In addition, alternative data may include proxies for protected classes that might also bias ML models. The Equal Credit Opportunity Act (ECOA; 15 U.S.C. §§1691-1691f) generally prohibits discrimination in credit transactions based upon certain protected classes, including sex, race, color, national origin, religion, marital status, age, and “because all or part of the applicant’s income derives from any public assistance program.” Questions exist about how lenders can comply with ECOA and other fair lending laws when using ML models for loan underwriting. Underserved Consumers and Access to CreditIn the United States, robust consumer credit markets allow most consumers to access financial services and credit products to meet their needs in traditional financial markets. However, consumers who have difficulty entering the traditional credit reporting system face challenges accessing many consumer credit products, because lenders are unable to assess their creditworthiness. Limited credit history is correlated with age, income, race, and ethnicity, and many of these consumers are young. Automated underwriting and ML models may expand credit access or make credit less expensive for some consumers. In particular, these technologies may increase financial inclusion for younger consumers, who may be more likely to have limited credit histories in the credit reporting system. Data Privacy, Security, and TransparencyIn credit underwriting, ML models often access sensitive consumer financial data, and the increase in digital data collection raises greater privacy and cybersecurity concerns. These data practices raise questions over what consumer information is appropriate to collect and use for loan underwriting. Laws such as the Fair Credit Reporting Act (FCRA; 15 U.S.C. §1681) and the Gramm-Leach-Bliley Act (GLBA; P.L. 106-102) impose requirements on firms that use consumer data for credit underwriting. As data use in consumer financial services has grown, some have debated whether the scope of these laws should be expanded.CRS ResourcesCRS Report R47475, Consumer Finance and Financial Technology (Fintech), coordinated by Cheryl R. Cooper.CRS Report R46795, Artificial Intelligence: Background, Selected Issues, and Policy Considerations, by Laurie A. Harris.CRS In Focus IF11630, Alternative Data in Financial Services, by Cheryl R. Cooper.CRS Report R44125, Consumer Credit Reporting, Credit Bureaus, Credit Scoring, and Related Policy Issues, by Cheryl R. Cooper and Darryl E. Getter.CRS In Focus IF10031, Introduction to Financial Services: The Consumer Financial Protection Bureau (CFPB), by Cheryl R. Cooper and David H. Carpenter.CRS In Focus IF11195, Financial Innovation: Reducing Fintech Regulatory Uncertainty, by David W. Perkins, Cheryl R. Cooper, and Eva Su.",https://www.congress.gov/crs_external_products/IF/PDF/IF12399/IF12399.1.pdf
IF12497,2023-09-18T04:00:00Z,2025-06-21T03:06:33Z,Semiconductors and Artificial Intelligence,"The increasing popularity of artificial intelligence (AI) has drawn congressional attention, and many Members are considering proposals to regulate the quickly evolving landscape. Technical progress in AI has been enabled in large part by advances in the underlying computational hardware—also known as semiconductors, integrated circuits, microelectronics, or simply chips—that offer increased processing power to improve the development of AI systems. This In Focus describes the types of semiconductors used in AI, concerns related to their supply chains, and challenges for the regulation of semiconductors to promote U.S. competitiveness in AI.Artificial Intelligence ModelsAI refers broadly to computational systems that can learn from data and make decisions such as predictions, recommendations, or classifications. AI systems can be implemented for diverse applications, including speech/visual recognition, autonomous driving, robotic process automation, and as virtual assistants. A popular class of AI systems is deep neural networks, which use algorithms, or models, to mimic neurons in the brain to identify complex patterns. This AI model typically involves two stages: training and inference. In the training phase, the model is fed data that can be labeled (e.g., thousands of pictures of dogs to learn all the variations of dogs) or unlabeled to identify patterns. In the inference phase, the trained model is used to enable predictions and guide decisions, such as autonomous driving systems recognizing dogs as obstacles to avoid. The training phase typically requires the most computational power.Generally, the accuracy of AI models increases with training on larger amounts of data, which in turn requires more computational power. Popular large language models, such as GPT-3, are trained on billions or trillions of text data to process and generate text. Given the large data sets associated with AI models, some of the largest AI models can take weeks or months to train, using thousands of chips and costing millions of dollars. These high costs are due in large part to the electricity required to operate the hardware.  Semiconductor Use in AI ModelsSemiconductors are tiny electronic devices designed to enable functions such as processing, storing, sensing, and moving data or signals. AI models employ different types of chips, including memory chips to store large amounts of data and logic chips to process the data. According to forecasts from Gartner, revenues from semiconductors used in AI may increase rapidly from around $44 billion in 2022 to $120 billion in 2027. Early AI models used commercial, off-the-shelf logic chips called central processing units (CPUs) for training and inference. Although CPUs are still sufficient for inference, leading AI models now primarily train using graphics processor units (GPUs) originally designed for video rendering. GPUs enable parallel processing of information; by contrast, CPUs process information serially. Parallel processing allows the AI model to train faster using large amounts of data by dividing tasks and executing them simultaneously. Additionally, many chip design firms are increasingly offering custom logic chips designed for particular applications, including AI training, called application-specific integrated circuits (ASIC) or accelerators. Logic chips used for AI applications are also referred to generally as AI chips. To train the largest AI models, many logic chips are connected together in large clusters with other semiconductor hardware (e.g., memory and networking chips) in data centers or supercomputing facilities. For example, Meta is building a supercomputer for AI research that is anticipated to contain 16,000 GPUs, and a startup called Inflection AI is building a cluster of 22,000 GPUs for its AI model. Some supercomputers built by private firms such as Meta, Tesla, and NVIDIA are larger than many nationally owned supercomputers around the world.Companies that train AI models may purchase and maintain their own chip hardware infrastructure or may train their models remotely using the cloud by paying fees to access the hardware they need. According to the Federal Trade Commission, “cloud services can be expensive and are currently provided by only a handful of firms, raising the risk of anticompetitive practices.” Top cloud service providers in the United States for AI applications include Amazon Web AI Services, Microsoft Azure AI, and Google Cloud AI. AI training typically benefits from improving two technical parameters for AI chips: higher processing power and faster chip-to-chip transfer speeds. A common metric used to market the processing power of different AI chips is a measurement of the number of mathematical operations a chip can do in one second, calculated in trillions of operations per second (TOPS). Chip-to-chip transfer speeds are generally reported by measuring how fast a chip can send information, or bytes, into and out of the chip in gigabytes per second (1 gigabyte is 1 billion bytes). Many large AI models, such as GPT models from OpenAI, and leading AI research papers do not explicitly report the amount of computational power used to train the AI model. Additionally, there are no standard methods or tools to measure the amount of computational power used to train AI models, as TOPS may be calculated differently by different companies and may not be the most optimal metric to evaluate and compare AI models. Transparency in computing usage for AI training and standard methods for measuring computing power globally may support regulatory efforts for AI. AI Chip Design and ManufacturingU.S.-headquartered companies, both established firms and start-ups, lead globally in the design of specialized logic chips for AI applications. However, the large majority of these chip-design firms rely wholly on contract manufacturing services to produce and package their designs. As the highest performance AI chips require the most advanced manufacturing processes in the world, the majority of AI chip designers rely on the two logic chip manufacturing firms currently capable of producing their designs: Taiwan Semiconductor Manufacturing Company (TSMC) and Samsung.  The top AI chip designer by revenue and usage in AI research is U.S.-headquartered NVIDIA, one of the first companies to mass market GPUs in the early 2000s. Leading GPU products from NVIDIA used in AI applications, in order of increasing computational power, are marketed by the names V100 (2017), A100 (2020), and H100 (2022). Each successor chip can transfer information into and out of the chip faster than its predecessor, enabling higher-speed communications between large clusters of chips and faster AI training. These higher performance metrics may enable an AI model to train faster than it would using other commercially available GPUs and, in turn, may lead to relatively lower costs.   Top U.S.-headquartered AI chip design start-ups include, by company valuation, SambaNova, Cerebras, and Graphcore. Smaller entities such as start-ups often face challenges to prototyping and producing their designs due to the high cost of and limited access to contract manufacturing services from companies such as TSMC. As competitiveness in AI benefits from advancements in chip hardware, promoting access to prototyping and manufacturing services for U.S.-based firms may boost long-term innovation. Export Controls on AI ChipsIn October 2022, the Department of Commerce implemented controls that require licenses for exports to China and Macau of certain advanced logic and other chips that can be used for applications such as AI training and for building supercomputers. Controls apply to those logic chips with chip-to-chip transfer speeds of 600 gigabytes per second or more and computational power over 600 TOPS. Under this definition, exports of leading AI chips, including NVIDIA’s A100 and H100, to China and Macau are restricted. In recent years, China accounted for about a quarter of total annual revenues for NVIDIA. In November 2022, NVIDIA began marketing an A800 chip, which had a lower chip-to-chip transfer speed of 400 gigabytes per second (compared with 600 gigabytes per second in the A100), to “provide alternative products not subject to the new license requirements” to customers in China, according to its annual report. Similarly, in March 2023, NVIDIA marketed an H800 chip that does not require a license as an alternative to the newest H100 products, which fall under the controls. Additionally, the October 2022 export controls restrict chip manufacturing facilities globally from manufacturing certain advanced chips for Chinese-headquartered chip design firms without a license if the manufacturer uses U.S.-origin technology or software (i.e., Advanced Computing Foreign Direct Product Rule). As the United States is a global leader in the production of semiconductor manufacturing equipment, this rule would apply to most foreign chip manufacturing firms, including TSMC, which previously produced advanced chips for Chinese AI chip design companies such as Biren. The rules require licenses to export certain advanced manufacturing equipment to chip manufacturers in China and Macau to impede the manufacturers’ ability to produce advanced chips.The export controls are designed to limit the ability of China and Macau to buy or produce certain advanced chips that can be used for AI applications. However, there are no controls on Chinese AI firms to use cloud service providers inside or outside of the country to train AI models. Selected Federal Actions and Considerations for CongressIn January 2021, Congress enacted the National Artificial Intelligence Initiative Act of 2020 (Division E of P.L. 116-283), which seeks to advance U.S. leadership in AI research and development. Part of the act seeks to establish a roadmap for a National AI Research Resource, a shared research infrastructure for AI researchers and students. Directed by Executive Order 13859, the National Institute of Standards and Technology conducted a study that recommended the federal government “commit to deeper, consistent, long-term engagement in AI standards development activities,” including the development of “metrics to quantifiably measure and characterize AI technologies, including but not limited to aspects of hardware and its performance.” In August 2022, President Biden signed P.L. 117-167, known as the CHIPS and Science Act. The act appropriated $39 billion to expand domestic semiconductor manufacturing capacity and $11 billion for research and development of next-generation semiconductor technologies. Congress may exercise its oversight authority with respect to the effectiveness of expanding domestic manufacturing capacity for advanced logic chips and improving manufacturing accessibility for smaller entities. Additionally, as many AI models are trained using cloud services, Congress may consider export control reforms that enable the Department of Commerce to exercise regulatory authority over providers of cloud services that sell access to large amounts of computational power. A previous version of this In Focus was authored by Manpreet Singh.",https://www.congress.gov/crs_external_products/IF/PDF/IF12497/IF12497.1.pdf
IN12289,2023-12-15T05:00:00Z,2025-06-21T02:35:46Z,Law Enforcement Use of Artificial Intelligence and Directives in the 2023 Executive Order,"The use of artificial intelligence (AI) has expanded in a variety of arenas, including by law enforcement. AI has been broadly conceptualized as computerized systems operating in ways often thought to require human intelligence. It is defined in the U.S. Code (15 U.S.C. §9401(3)) asa machine-based system that can, for a given set of human-defined objectives, make predictions, recommendations or decisions influencing real or virtual environments. Artificial intelligence systems use machine and human-based inputs to-(A) perceive real and virtual environments;(B) abstract such perceptions into models through analysis in an automated manner; and(C) use model inference to formulate options for information or action.AI involves a host of technologies and applications. In the law enforcement realm, researchers note that while the use of AI is not yet widespread, existing tools may be enhanced with AI to expand law enforcement capabilities and increase their efficiency. Examples include the following:Automated license plate readers can be leveraged to employ machine, or computer, vision for capabilities such as automating the issuance of red-light violation tickets.Security cameras outfitted with certain AI-embedded hardware can be used for real-time facial recognition of potential suspects.Facial recognition technology and text analysis tools can be enhanced with AI to scan online advertisements to help identify potential crimes such as human trafficking.In addition to gunshot detection technology that can detect shots fired, security cameras can be outfitted with AI-enhanced software to detect weapons and alert police before shots are fired.AI redaction capabilities can be used to reduce possible bias in officers’ narratives by removing certain identifying characteristics of suspects and victims—such as race—that could influence charges brought by prosecutors.Body-worn cameras can use AI software to redact or blur faces or sensitive footage before it is released to the public.Automated speech recognition software can use AI to help properly identify speakers’ voices in audiovisual materials such as witness testimonies or interrogations.First responders use computer aided dispatch (CAD) systems to capture data that inform decisions on resource deployment; AI-enhanced CAD systems can improve resource allocation by using historical data, making predictions, and automating workflow.AI can be used along with predictive policing models to help identify individuals or places most at risk of being involved in crime.Law enforcement agencies can employ AI to enhance its communications with the public; they can use chatbots to respond to questions and push out emergency information.A number of concerns have been raised about law enforcement use of AI, including whether its use perpetuates biases; one criticism is that the data on which the software are trained contain bias, thus training bias into the AI systems. Another concern is whether reliance on AI technology may lead police to ignore contradictory evidence. Policymakers may consider increased oversight over police use of AI systems to help evaluate and alleviate some of the shortcomings.On October 30, 2023, President Biden issued Executive Order (E.O.) 14110 on the Safe, Secure, and Trustworthy Development and Use of Artificial Intelligence. This E.O. advances a government-wide approach to “governing the development and use of AI safely and responsibly” and directs efforts in AI policy areas involving safety and security, innovation and competition, worker support, equity and civil rights, individual protections, privacy protections, federal AI use, and international leadership. E.O. 14110 acknowledges the risk of AI exacerbating discrimination and directs federal law enforcement in various ways. (In doing so, it references accountability focused directives for federal law enforcement previously outlined in the May 25, 2022, E.O. 14074 on Advancing Effective, Accountable Policing and Criminal Justice Practices to Enhance Public Trust and Public Safety.) Directives in E.O. 14110 include the following:The Attorney General (AG) shall coordinate and support enforcement of federal laws addressing discrimination and violations of civil rights and civil liberties related to AI. The Department of Justice’s Civil Rights Division shall also coordinate with other federal civil rights offices to assess how their offices can prevent and address discrimination in automated systems—including algorithmic discrimination.The AG, with the Homeland Security Secretary and Office of Science and Technology Policy Director, shall submit a report to the President on the use of AI in the criminal justice system, including how AI can enhance law enforcement efficiency and accuracy, consistent with privacy, civil rights, and civil liberties protections. The report should also recommend best practices for law enforcement, including guidance on AI use, to address concerns outlined in E.O. 14074 with respect to law enforcement use of “facial recognition technology, other technologies using biometric information, and predictive algorithms, as well as data storage and access regarding such technologies.”The interagency working group established by E.O. 14074 shall share best practices for recruiting law enforcement professionals with AI expertise and training them on responsible AI use. The AG, along with the Homeland Security Secretary, may review these and recommend best practices for state, local, tribal, and territorial law enforcement.The AG shall review the Justice Department’s capacity to “investigate law enforcement deprivation of rights under color of law resulting from the use of AI,” including through increasing or improving training for federal law enforcement officers and prosecutors.Policymakers conducting oversight of executive branch activities to ensure that AI is used in a fair and equitable manner may examine not only these elements of E.O. 14110 that specifically relate to federal law enforcement but also other elements—such as the development of industry standards on AI—that may in turn affect law enforcement use of AI. They may also explore whether there should be specific standards for AI use in the criminal justice sector or AI-specific requirements for criminal justice entities receiving federal grants. Additionally, policymakers may continue to debate law enforcement use of specific AI technologies in its toolbox such as facial recognition technology.",https://www.congress.gov/crs_external_products/IN/PDF/IN12289/IN12289.2.pdf
LSB11097,2023-12-28T05:00:00Z,2025-06-21T02:48:24Z,Section 230 Immunity and Generative Artificial Intelligence,"Over the past year, technology companies have expanded access to services capable of creating content using artificial intelligence (AI). In February 2023, Microsoft announced an “all new, AI-powered Bing search engine” that “can help you write an email, create a 5-day itinerary for a dream vacation to Hawaii, with links to book your travel and accommodations, prep for a job interview or create a quiz for trivia night.” Google introduced Bard, an “experimental conversational AI service,” the same month. Applications such as these that are capable of generating new content like text, images, and videos form a subset of AI applications often referred to as “generative AI.”As access to AI tools has expanded and the underlying models have become more powerful, Congress has shown interest in regulating AI models. Committees have held hearings, and Members have introduced bills to regulate AI (or announced a framework for doing so). Some Members have called for a task force or commission to recommend rules. The executive branch has also weighed in. The White House issued an Executive Order, in October 2023, tasking agencies with actions to address the advent of AI models.Generative AI poses unique policy and legal issues. One question raised by the introduction of generative AI products is the extent to which companies that provide the products could be held liable for illegal content generated by the AI. The answer likely depends in part on an existing legal framework: Section 230 of the Communications Act of 1934, a federal statute that, subject to some exceptions, immunizes interactive computer service providers from being sued as the publisher or speaker of information provided by another party. If this immunity extends to claims based on an output from a generative AI product, plaintiffs defamed by an AI output (for example) may be barred from suing a company that provided the AI product. The potential application of Section 230 to generative AI has already garnered comments at a Supreme Court oral argument and from Section 230’s primary authors. This Sidebar discusses how Section 230 might apply to legal claims involving generative AI products.Section 230Section 230 creates a federal immunity for publishing another person’s content online, as explained in a longer CRS report. Specifically, Section 230, enacted in the Communications Decency Act of 1996, prevents providers and users of “interactive computer services” from being held liable—that is, legally responsible—as the “publisher or speaker” of information provided by another person. “Interactive computer service” is defined as any service that “provides or enables computer access by multiple users to a computer server.” This broad term encompasses services such as Facebook, Google, and Amazon. There are exceptions to Section 230 immunity, such as for intellectual property law. Section 230 only provides immunity for content provided by another person and does not apply if a provider or user of an interactive computer service helped create or develop the content. Although the Supreme Court has never interpreted Section 230, many federal and state courts have considered when a lawsuit would attempt to hold a defendant liable for another’s content or for the defendant’s own content. A number of courts have settled on a “material contribution” test, under which Section 230 immunity does not apply if the provider or user materially contributed to the alleged unlawfulness of the content. This inquiry is highly fact-specific. The U.S. Court of Appeals for the Ninth Circuit, for example, decided in one case that Section 230 barred housing discrimination claims against the operator of the website Roommates.com based on one portion of the website but did not bar claims based on a different portion of the site. The court said an open text field where roommate seekers could describe “what [they] are looking for in a roommate” did not materially contribute to discrimination, even if users’ descriptions ended up facilitating discriminatory searches. In contrast, the court held that “search and email systems [designed] to limit the listings available to subscribers based on sex, sexual orientation and presence of children” did materially contribute to alleged discrimination. The court suggested sites could provide “neutral tools” to post user content so long as the tools do not materially contribute to illegal activity. Citing this “neutral tools” analysis, some federal courts of appeals have held that Section 230 barred lawsuits against service providers for promoting harmful content through algorithms, where the algorithms used objective factors that treated the harmful content similarly to other content. Beyond content recommendation algorithms, courts have held that other choices about how to present material—and even actions that rearrange or slightly edit material—can also be immunized by Section 230. In O’Kroley v. Fastcase, Inc., a federal appeals court concluded that Section 230 barred a defamation lawsuit challenging the way Google presented its search results. The plaintiff alleged that when he searched his name, one entry on the search results page inaccurately suggested that he had been involved in a case regarding indecency with a child. The court held that even though Google “performed some automated editorial acts on” content provided by other parties, “such as removing spaces and altering font,” these alterations did not materially contribute to the alleged defamatory nature of the content. In contrast, in FTC v. Accusearch Inc., a federal appeals court held Section 230 immunity did not apply because the website operator that was sued had helped to develop third-party information. The website sold information contained in telephone records. Although the website acquired the records from other parties, it solicited and paid for them. The legal harm came from the publication of the records: the FTC alleged the website operator violated a federal provision limiting disclosure of certain personal information. The court said the website operator was responsible for developing the information and thus not protected by Section 230, as it “contributed mightily to the unlawful conduct” by paying researchers to acquire confidential records with the intent of making them public contrary to federal law. Even if Section 230 does not apply, a service provider will only be liable in a lawsuit if a plaintiff can prove their underlying legal claims. For example, if a copyright lawsuit falls within the Section 230 exception for intellectual property law, the plaintiff will still have to prove the provider in fact violated copyright law. Other CRS products may be relevant to that inquiry. For example, other products provide an introduction to tort law and discuss generative AI and copyright law or campaign advertising.Generative AI“Artificial intelligence” is a broad term referring to computerized systems that act in ways commonly thought to imitate human intelligence. Generative AI is a type of AI that uses machine learning to generate new content, as discussed in this In Focus. For instance, a user can provide a text prompt that the generative AI uses to create an image or other content. Machine learning algorithms allow computers to learn at least partially without explicit instructions. Programmers gather and prepare training data, then they feed that data into a machine learning model. There are different types of generative AI, but applications based on large language models, such as OpenAI’s ChatGPT, were trained by providing the model with massive amounts of existing data scraped from the internet. (This CRS report discusses some of the data privacy concerns with generative AI.) Accordingly, when generative AI creates new content, it draws from this existing information created by other parties and attempts to match the style and content of the underlying data. However, the new content may not be identical to the training data. Generative AI may sometimes produce so-called “hallucinated” outputs that, for instance, go beyond the scope of the training data or have incorrectly decoded that data. The generative outputs are also influenced by the specific text of the prompt itself—because the algorithms generate content based on statistical probabilities that the underlying training data are associated with the words in the prompt, changes to the words in the prompt can change the output. Even the same prompt provided to a generative AI system multiple times can result in different outputs. Potential Application of Section 230 to Generative AICourts have not yet decided whether or how Section 230 may be used as a defense against claims based on outputs from recently released generative AI products, but they may soon be asked to address the issue. Outputs from generative AI products have already led to lawsuits. In June 2023, for example, a radio host sued OpenAI, alleging that ChatGPT defamed him. In a second defamation case, a plaintiff alleged that searching for his name on Microsoft’s Bing search engine returned an AI-generated summary that commingled facts about him with facts about a different individual who had a similar name and who once pleaded guilty to seditious conspiracy. Although no defendant in these cases has yet raised Section 230 as a defense, the claims are similar to those that have given rise to Section 230 defenses in the past. In each case, the plaintiff alleges that a defendant published information using a tool that relies at least in part on user inputs and data created by other parties. At least one commentator has asserted that generative AI companies might attempt to invoke Section 230 in similar circumstances. Even if raised, the courts deciding these cases may not address the issue. The cases could settle or be resolved on the merits or another defense. Still, these or future lawsuits alleging that generative AI outputs are defamatory, negligent, or cause other legal harms may test whether the providers of the tools or users who requested those outputs materially contributed to the harmfulness of the challenged content such that Section 230 immunity would not apply.If Section 230 were to be invoked in such circumstances, past cases suggest Section 230 would be applied in a fact-specific manner. Thus, a Section 230 inquiry in a lawsuit challenging any given AI-generated output would likely depend on the particular legal claim and underlying facts. Not all generative AI products function the same way, and not all legal claims would necessarily turn on the same aspects of a given product. As one group of scholars asserts, generative AI products “operate on something like a spectrum between a retrieval search engine (more likely to be covered by Section 230) and a creative engine (less likely to be covered).” Section 230’s application could therefore vary across different generative AI products, different applications of a single product, and different legal claims about an application or product. As such, commentators have made different predictions about how courts would resolve Section 230 defenses based in part on which aspects of generative AI the arguments focus on. Some commentators contend that “AI programs’ output is composed by the programs themselves,” so the AI providers should be viewed as information creators or developers that receive no Section 230 immunity. Large language models, for example, can “draft[] text on a topic in response to a user request or develop[] text to summarize the results of a search inquiry.” A commentator has argued that this aspect of the technology would likely lead courts to conclude that large language models develop text themselves. A large language model’s output can “hallucinate,” creating brand new “text on a topic” that no other party has ever written. If generated content contains claims or assertions that do not appear in its training data, the claims or assertions could be seen as entirely new information created by the providers rather than by another person. Even if an AI program assembles facts or claims from training data into new material that does not appear elsewhere on the internet, this may be viewed as similar to Accusearch, where the website operator was deemed “responsible for the development of the specific content that was the source of the alleged liability” and thus was unprotected by Section 230. On the other hand, another commentator argues that the current iteration of ChatGPT (as of March 2023) “is entirely driven by third-party input” and “does not invent, create, or develop outputs absent any prompting from an information content provider.” When it receives a prompt, ChatGPT “uses predictive algorithms and an array of data made up entirely of publicly available information online to respond to [the] user-created inputs.” Outside the context of Section 230, OpenAI has claimed that the “content” associated with any particular ChatGPT interaction includes both the machine-generated outputs and the user-generated inputs that prompted them. Courts focusing on these aspects of the product could consider ChatGPT analogous to search engines that use algorithms to generate lists and illustrative snippets of websites in response to user inputs. As discussed, federal appeals courts have held that Section 230 shields internet search engines from liability for claims based on the results returned by users’ searches.Autocomplete features are another potential analogy, given that some generative AI operates by predicting and assembling plausible text outputs that it associates with user prompts and patterns learned from training datasets. Although there is relatively little case law on point, two federal trial courts have held that Section 230 immunizes search engines from claims that auto-generated, suggested search terms were defamatory. The courts reasoned that the “auto-generated terms indicate[] only that other websites and users have connected plaintiff’s name’ with certain terms,” meaning that the allegedly harmful content was created by another party. This reasoning could apply to an AI product that completes a sentence by “predict[ing] probabilities for the next plausible word or phrase given previous words or phrases.”Similarly, some courts have held, under the neutral tools test, that other algorithmic processes do not develop content for the purposes of Section 230. For example, the U.S. Court of Appeals for the Second Circuit held that Facebook does not develop or create content that its algorithms arrange and display on a user’s page when the algorithms “take the information provided by . . . users and match’ it to other users . . . based on objective factors applicable to any content.” As mentioned, a large language model can be “trained simply to predict probabilities for the next plausible word or phrase given previous words or phrases.” Some scholars have argued that this sort of probabilistic compilation could likewise be considered a tool that operates neutrally, regardless of the particular content to which it is being applied. The same algorithmic process that generates an output containing defamation or other illegal content in response to some user interactions could generate lawful outputs in response to other interactions. The neutral tools test, however, has been criticized by some, including with respect to how it might apply to AI products. A court could deny Section 230 immunity without resorting to the neutral tools test if it concluded in a given case that an AI output was created by the AI provider rather than information created by another person. In all of these potential analyses, details will matter. Regardless of how the contours of the analysis are ultimately defined, current case law suggests that courts applying Section 230 to claims involving generative AI products would need to look closely at how the particular AI product at issue generates an output and what aspect of the output the plaintiff alleges to be illegal. Because generative AI products are not all the same, are likely to continue to evolve, and can rely on data and inputs from disparate sources, Section 230 analysis may lead to different outcomes in different cases.Considerations for CongressAs Congress contemplates regulating generative AI, it may consider whether any proposals that would impose liability on generative AI companies would potentially conflict with Section 230 immunity. If so, Congress might consider creating Section 230 carveouts in any new legislation. Without such a carveout, courts may construe a law that imposes civil liability on generative AI companies to be limited by Section 230. For instance, Congress could create a new private right of action for harms stemming from generative AI outputs. The new right of action could define the elements a plaintiff would have to prove to establish liability and could specify whether Section 230 provides a defense. Congress might also consider creating new immunities to legal claims that relate to generative AI outputs. A new immunity could preclude liability regardless of whether Section 230 may also apply.In addition, Congress could amend Section 230 to address generative AI directly. This sort of amendment could extend immunity to claims arising from generative AI products, withhold immunity from such claims, or extend immunity to certain types of claims and withhold it from others. A bill introduced in the 118th Congress (S. 1993), for example, would withhold Section 230 immunity “if the conduct underlying the claim or charge involves the use or provision of generative artificial intelligence by the interactive computer service.” This bill would remove immunity not only for generative AI providers but also in any claim involving a provider’s use of generative AI. Alternatively, Congress could wait to see how courts address defenses under the current version of Section 230 in cases based on outputs from generative AI.Other proposals to more broadly amend Section 230 could also have implications for generative AI. For instance, the DISCOURSE Act (S. 921) would effectively create an exception for certain service providers that modify or alter others’ information. This approach could sweep in generative AI that edits third-party content in creating new outputs. Congress’s ability to regulate in this area could be limited by the First Amendment’s Free Speech Clause. Some scholars have argued that generative AI output is protected by the First Amendment, either because the creators use the program as a means of expression or because the program’s users have a right to use generative AI as a method of creating or receiving expression. The Supreme Court has ruled that “the creation and dissemination of information are speech within the meaning of the First Amendment,” such that even factual information (and sometimes false information) can be constitutionally protected. These cases suggest a law creating or allowing liability for using generative AI could implicate free speech interests. If Section 230 did not bar a lawsuit, whether under existing interpretations of the law or because Congress created a new exception, the protections of the First Amendment might still apply.A law that affects speech is not necessarily unconstitutional, however. First Amendment protections are not absolute. Instead, courts apply different types of constitutional scrutiny depending on the type of speech being regulated and how the regulation affects that speech. A law amending when Section 230 immunity applies might be scrutinized differently than a law more directly regulating generative AI activity. Further, for example, laws that regulate speech based on its content are generally subject to stricter constitutional scrutiny than content-neutral regulations. Thus, the constitutionality of a law regulating generative AI, including through an amendment to Section 230, may depend in part on whether the law targets the function of a generative AI program or its expressive output and on whether it targets specific types of AI outputs based on their content.",https://www.congress.gov/crs_external_products/LSB/PDF/LSB11097/LSB11097.2.pdf
R46795,2021-05-19T04:00:00Z,2025-06-20T22:59:02Z,"Artificial Intelligence: Background, Selected Issues, and Policy Considerations","The field of artificial intelligence (AI)—a term first used in the 1950s—has gone through multiple waves of advancement over the subsequent decades. Today, AI can broadly be thought of as computerized systems that work and react in ways commonly thought to require intelligence, such as the ability to learn, solve problems, and achieve goals under uncertain and varying conditions. The field encompasses a range of methodologies and application areas, including machine learning (ML), natural language processing, and robotics. In the past decade or so, increased computing power, the accumulation of big data, and advances in AI techniques have led to rapid growth in AI research and applications. Given these developments and the increasing application of AI technologies across economic sectors, stakeholders from academia, industry, and civil society have called for the federal government to become more knowledgeable about AI technologies and more proactive in considering public policies around their use.Federal activity addressing AI accelerated during the 115th and 116th Congresses. President Donald Trump issued two executive orders, establishing the American AI Initiative (E.O. 13859) and promoting the use of trustworthy AI in the federal government (E.O. 13960). Federal committees, working groups, and other entities have been formed to coordinate agency activities, help set priorities, and produce national strategic plans and reports, including an updated National AI Research and Development Strategic Plan and a Plan for Federal Engagement in Developing Technical Standards and Related Tools in AI. In Congress, committees held numerous hearings, and Members introduced a wide variety of legislation to address federal AI investments and their coordination; AI-related issues such as algorithmic bias and workforce impacts; and AI technologies such as facial recognition and deepfakes. At least four laws enacted in the 116th Congress focused on AI or included AI-focused provisions.The National Defense Authorization Act for FY2021 (P.L. 116-283) included provisions addressing various defense- and security-related AI activities, as well as the expansive National Artificial Intelligence Initiative Act of 2020 (Division E). The Consolidated Appropriations Act, 2021 (P.L. 116-260) included the AI in Government Act of 2020 (Division U, Title I), which directed the General Services Administration to create an AI Center of Excellence to facilitate the adoption of AI technologies in the federal government. The Identifying Outputs of Generative Adversarial Networks (IOGAN) Act (P.L. 116-258) supported research on Generative Adversarial Networks (GANs), the primary technology used to create deepfakes. P.L. 116-94 established a financial program related to exports in AI among other areas. AI holds potential benefits and opportunities, but also challenges and pitfalls. For example, AI technologies can accelerate and provide insights into data processing; augment human decisionmaking; optimize performance for complex tasks and systems; and improve safety for people in dangerous occupations. On the other hand, AI systems may perpetuate or amplify bias, may not yet be fully able to explain their decisionmaking, and often depend on vast datasets that are not widely accessible to facilitate research and development (R&D). Further, stakeholders have questioned the adequacy of human capital in both the public and private sectors to develop and work with AI, as well as the adequacy of current laws and regulations for dealing with societal and ethical issues that may arise. Together, such challenges can lead to an inability to fully assess and understand the operations and outputs of AI systems—sometimes referred to as the “black box” problem.Because of these questions and concerns, some stakeholders have advocated for slowing the pace of AI development and use until more research, policymaking, and preparation can occur. Others have countered that AI will make lives safer, healthier, and more productive, so the federal government should not attempt to slow it, but rather should give broad support to AI technologies and increase federal AI funding.In response to this debate, Congress has begun discussing issues such as the trustworthiness, potential bias, and ethical uses of AI; possible disruptive impacts to the U.S. workforce; the adequacy of U.S. expertise and training in AI; domestic and international efforts to set technological standards and testing benchmarks; and the level of U.S. federal investments in AI research and development and how that impacts U.S. international competitiveness. Congress is likely to continue grappling with these issues and working to craft policies that protect American citizens while maximizing U.S. innovation and leadership.",https://www.congress.gov/crs_external_products/R/PDF/R46795/R46795.2.pdf
R45178,2020-11-10T05:00:00Z,2025-06-20T22:32:26Z,Artificial Intelligence and National Security,"Artificial intelligence (AI) is a rapidly growing field of technology with potentially significant implications for national security. As such, the United States and other nations are developing AI applications for a range of military functions. AI research is underway in the fields of intelligence collection and analysis, logistics, cyber operations, information operations, command and control, and in a variety of semiautonomous and autonomous vehicles. Already, AI has been incorporated into military operations in Iraq and Syria. Congressional action has the potential to shape the technology’s development further, with budgetary and legislative decisions influencing the growth of military applications as well as the pace of their adoption.AI technologies present unique challenges for military integration, particularly because the bulk of AI development is happening in the commercial sector. Although AI is not unique in this regard, the defense acquisition process may need to be adapted for acquiring emerging technologies like AI. In addition, many commercial AI applications must undergo significant modification prior to being functional for the military. A number of cultural issues also challenge AI acquisition, as some commercial AI companies are averse to partnering with the Department of Defense (DOD) due to ethical concerns, and even within the department, there can be resistance to incorporating AI technology into existing weapons systems and processes.Potential international rivals in the AI market are creating pressure for the United States to compete for innovative military AI applications. China is a leading competitor in this regard, releasing a plan in 2017 to capture the global lead in AI development by 2030. Currently, China is primarily focused on using AI to make faster and more well-informed decisions, as well as on developing a variety of autonomous military vehicles. Russia is also active in military AI development, with a primary focus on robotics. Although AI has the potential to impart a number of advantages in the military context, it may also introduce distinct challenges. AI technology could, for example, facilitate autonomous operations, lead to more informed military decisionmaking, and increase the speed and scale of military action. However, it may also be unpredictable or vulnerable to unique forms of manipulation. As a result of these factors, analysts hold a broad range of opinions on how influential AI will be in future combat operations. While a small number of analysts believe that the technology will have minimal impact, most believe that AI will have at least an evolutionary—if not revolutionary—effect. Military AI development presents a number of potential issues for Congress:What is the right balance of commercial and government funding for AI development?How might Congress influence defense acquisition reform initiatives that facilitate military AI development?What changes, if any, are necessary in Congress and DOD to implement effective oversight of AI development?How should the United States balance research and development related to artificial intelligence and autonomous systems with ethical considerations?What legislative or regulatory changes are necessary for the integration of military AI applications?What measures can Congress take to help manage the AI competition globally?",https://www.congress.gov/crs_external_products/R/PDF/R45178/R45178.9.pdf
