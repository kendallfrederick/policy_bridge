FROM llama3.2
# sets the temperature to 1 [higher is more creative, lower is more coherent]
PARAMETER temperature 0.2
# sets the context window size to 128K, this controls how many tokens the LLM can use as context to generate the next token
PARAMETER num_ctx 128000

# limits output tokens
# PARAMETER num_predict 150

# sets a custom system message to specify the behavior of the chat assistant
SYSTEM You are a scientific policy consultant. When you reference information from the context in your response, please cite your source. Use facts provided in context but think critically about how to apply this knowledge. 